{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Answers_similarity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t2Oqtn5jhgd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwieW9oajoP0",
        "outputId": "ad8e18ee-ba1d-4dcc-f991-4f408ad8080f"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "!pip install pytorch-pretrained-bert\n",
        "from pytorch_pretrained_bert import BertModel"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 7.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 40.7MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n",
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/c0/8af2139d5658eccde11f45fd9d27046edb286fd60f5371e27870612287bd/boto3-1.17.111-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.9.0+cu102)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.6MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.111\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/56/64570ac92c7cb88ad731dea4da4a83d3edc9f00a13a969ad826354ba5a58/botocore-1.20.111.tar.gz (7.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9MB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.111->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.111->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Building wheels for collected packages: botocore\n",
            "  Building wheel for botocore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for botocore: filename=botocore-1.20.111-py2.py3-none-any.whl size=7695032 sha256=8f26a5ee5e69e6c6432c07042bb10f577c5d3aa1d168a71c59beb7996a83bd55\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/5f/dd/9021b3f78dc76c95f97ea9cd1798aa6da9bc1a61fe7d1bb9fa\n",
            "Successfully built botocore\n",
            "\u001b[31mERROR: botocore 1.20.111 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.111 botocore-1.20.111 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKw0xxDFjqIL",
        "outputId": "5a9b4f33-4388-4fb0-fd83-0b35ffd419a9"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, set our device to GPU. use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")   "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uEuLZhQjsaD",
        "outputId": "9933ac59-3900-4c89-eea9-4c3858d6d1b6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "W84OH690jvQu",
        "outputId": "86a666c1-4e5d-445d-ce07-d5aa41a94fc7"
      },
      "source": [
        "# data1 = pd.read_csv('/content/drive/My Drive/train_snli.txt', sep=\"\\t\", header=None)\n",
        "# data1.columns = [\"ans1\", \"ans2\", \"class\"]\n",
        "data = pd.read_csv('/content/drive/My Drive/Clean_train_data.csv')\n",
        "# frames = [data1, data2]\n",
        "# data = pd.concat(frames)\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "data.sample(10)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>ans1</th>\n",
              "      <th>ans2</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3000</th>\n",
              "      <td>What is a Confusion Matrix?</td>\n",
              "      <td>A confusion matrix is a technique for summariz...</td>\n",
              "      <td>Parametric models are those with a finite numb...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1868</th>\n",
              "      <td>Explain the following variant of Gradient Desc...</td>\n",
              "      <td>This is a type of gradient descent which updat...</td>\n",
              "      <td>the term \"stochastic\" comes from the fact that...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1941</th>\n",
              "      <td>Explain the following variant of Gradient Desc...</td>\n",
              "      <td>Mini-batch gradient descent is a variation of ...</td>\n",
              "      <td>With Dropout, the training process essentially...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1014</th>\n",
              "      <td>Explain the following variant of Gradient Desc...</td>\n",
              "      <td>can be used to predict probability as it produ...</td>\n",
              "      <td>Using the Gradient Decent optimization algorit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3746</th>\n",
              "      <td>Difference between BatchNorm and LayerNorm?</td>\n",
              "      <td>Batch normalization is applied on the neuron a...</td>\n",
              "      <td>BatchNorm: Normalize the batch direction and c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>Differentiate supervised and unsupervised deep...</td>\n",
              "      <td>Supervised learning is a system in which both ...</td>\n",
              "      <td>supervised learning is where I have the input ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2076</th>\n",
              "      <td>What is Tanh function?</td>\n",
              "      <td>convolutional neural network (CNN, or ConvNet)...</td>\n",
              "      <td>The tanh function, a.k.a. hyperbolic tangent f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3910</th>\n",
              "      <td>Explain the term constructor</td>\n",
              "      <td>A class constructor is a special member functi...</td>\n",
              "      <td>Encapsulation is used to hide the values or st...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>what do you know about output layer?</td>\n",
              "      <td>layer takes in the inputs which are passed in ...</td>\n",
              "      <td>The output layer is the simplest, data is made...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4170</th>\n",
              "      <td>What is computer software?</td>\n",
              "      <td>Software is a program that enables a computer ...</td>\n",
              "      <td>Software configuration management is a process...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Questions  ... class\n",
              "3000                       What is a Confusion Matrix?   ...     0\n",
              "1868  Explain the following variant of Gradient Desc...  ...     1\n",
              "1941  Explain the following variant of Gradient Desc...  ...     0\n",
              "1014  Explain the following variant of Gradient Desc...  ...     0\n",
              "3746        Difference between BatchNorm and LayerNorm?  ...     1\n",
              "133   Differentiate supervised and unsupervised deep...  ...     1\n",
              "2076                             What is Tanh function?  ...     0\n",
              "3910                       Explain the term constructor  ...     0\n",
              "827                what do you know about output layer?  ...     1\n",
              "4170                         What is computer software?  ...     0\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_S3u2K5j9DH",
        "outputId": "f75d80ca-192d-43c7-bfa6-94d0a94da1ee"
      },
      "source": [
        "# sentences_pairs = data[[\"sentence1\", \"sentence2\"]].values.astype(\"str\")\n",
        "sents_pairs = data[[\"ans1\", \"ans2\"]].values.astype(\"str\")\n",
        "labels = data[\"class\"].values\n",
        "print(sents_pairs.shape, labels.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4274, 2) (4274,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An5gKQdqkOb0"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s80G1UeQkQ1E",
        "outputId": "8f9c704a-5b59-48a1-a690-2a5ad6e8fb7a"
      },
      "source": [
        "sents_pairs[0]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network.Deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.',\n",
              "       'Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.'],\n",
              "      dtype='<U1005')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qUAoVFLnYut"
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "def clean2_text(text):\n",
        "  #remove some of stopwords as 'a, an, the'\n",
        "  txt = clean_text(text)\n",
        "  words = txt.split(' ')\n",
        "  aft_remove = [w for w in words if w not in ['a', 'an', 'the','of', 'that', 'which','in', 'at', 'for', 'where', 'when', 'to', 'from']]\n",
        "  return ' '.join(aft_remove)\n",
        "\n",
        "# p = clean_text(sents_pairs[0][0])\n",
        "# print(sents_pairs[0][0])\n",
        "# print(p)\n",
        "\n",
        "# cl_tx = clean2_text(p)\n",
        "# print(cl_tx)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R81wk1kWad9g"
      },
      "source": [
        "#cleaning the training data\n",
        "sentences_pairs = sents_pairs\n",
        "for i, sent in enumerate(sents_pairs):\n",
        "    sentences_pairs[i][0] = clean2_text(sent[0])\n",
        "    sentences_pairs[i][1] = clean2_text(sent[1])\n",
        "\n",
        "# print(sentences_pairs[0])    "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-XQCCaTqBQZ"
      },
      "source": [
        "# #know the maximum length of all sentences\n",
        "# def get_maximum(sent_pairs):\n",
        "#   max_len = 0\n",
        "#   # For every sentence...\n",
        "#   for sent in sent_pairs:\n",
        "#       # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "      \n",
        "#       input_ids = tokenizer.encode(sent[0], add_special_tokens=True)\n",
        "#       input_ids2 = tokenizer.encode(sent[1], add_special_tokens=True)\n",
        "#       # Update the maximum sentence length.\n",
        "#       max_len = max(max_len, len(input_ids) , len(input_ids2))\n",
        "#   return max_len\n",
        "\n",
        "# max_length = get_maximum(sentences_pairs)\n",
        "# print('Max sentence length: ', max_length)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2b4IGP7rGkq",
        "outputId": "36b6f222-36c4-42ea-8925-15ab17248bd1"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "\n",
        "def encoding_sentences(sentences_pairs, labels, max_length = 200):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  token_type_ids = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent_pair in sentences_pairs:\n",
        "\n",
        "      encoded_pair = tokenizer(  sent_pair[0], sent_pair[1], \n",
        "                                padding= 'max_length',  # Pad to max_length\n",
        "                                truncation= True,  # Truncate to max_length\n",
        "                                max_length= max_length,  \n",
        "                                return_tensors='pt'  # Return torch.Tensor objects\n",
        "      )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      input_ids.append(encoded_pair['input_ids'])\n",
        "      \n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      attention_masks.append(encoded_pair['attention_mask'])\n",
        "\n",
        "      # Add its token type id to the list\n",
        "      token_type_ids.append(encoded_pair['token_type_ids'])\n",
        "\n",
        "  # # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  return input_ids, attention_masks, token_type_ids, labels\n",
        "\n",
        "\n",
        "input_ids, attention_masks, token_type_ids, labels = encoding_sentences(sentences_pairs, labels, 128)\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences_pairs[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ['deep learning is part machine learning with algorithm inspired by structure and function brain is called artificial neural networkdeep learning is suited over range fields such as computer vision speech recognition natural language processing etc'\n",
            " 'deep learning is subset machine learning artificial intelligence has networks capable learning unsupervised data is unstructured or unlabeled also known as deep neural learning or deep neural network']\n",
            "Token IDs: tensor([  101,  2784,  4083,  2003,  2112,  3698,  4083,  2007,  9896,  4427,\n",
            "         2011,  3252,  1998,  3853,  4167,  2003,  2170,  7976, 15756,  2897,\n",
            "        26095,  2361,  4083,  2003, 10897,  2058,  2846,  4249,  2107,  2004,\n",
            "         3274,  4432,  4613,  5038,  3019,  2653,  6364,  4385,   102,  2784,\n",
            "         4083,  2003, 16745,  3698,  4083,  7976,  4454,  2038,  6125,  5214,\n",
            "         4083,  4895,  6342,  4842, 11365,  2098,  2951,  2003,  4895,  3367,\n",
            "        26134,  2030,  4895, 20470, 12260,  2094,  2036,  2124,  2004,  2784,\n",
            "        15756,  4083,  2030,  2784, 15756,  2897,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OxLsBT0rNIT",
        "outputId": "9e737b1f-b238-4ef8-908f-241e45d72a4d"
      },
      "source": [
        "print(input_ids.shape, attention_masks.shape, labels.shape)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4274, 128]) torch.Size([4274, 128]) torch.Size([4274])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yr52pMPrQVH",
        "outputId": "d22e4bbb-e6c8-43d3-8d41-26e86ffa06dd"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.6 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2,564 training samples\n",
            "  854 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImhthsCOE8M3"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "# take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # get batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97KalDv3FBJA"
      },
      "source": [
        "class Similarity_Model(nn.Module):\n",
        "    def __init__(self, output_dim, n_layers, hidden_dim, freeze_bert):\n",
        "        super(Similarity_Model,self).__init__()\n",
        " \n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "        \n",
        "        #bert Model and Freeze the BERT model\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_model.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        #LSTM layers\n",
        "        # self.lstm = nn.LSTM(768, hidden_dim, n_layers, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # linear layer\n",
        "        self.fc = nn.Linear(768, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_masks, token_type_ids, hidden):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # print(\"batch\" , batch_size)\n",
        "\n",
        "        sequence_output, pooled_output = self.bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids) \n",
        "\n",
        "        # print(\"seq  \" , len(sequence_output))\n",
        "        \n",
        "        # lstm_out, hidden = self.lstm(sequence_output[0], hidden)\n",
        "        \n",
        "        # # print(\"lstm_out  \" , lstm_out.shape)\n",
        "\n",
        "        # lstm_out = lstm_out.permute(0,2,1)\n",
        "\n",
        "        # # print(\"lstm_out  \" , lstm_out.shape)\n",
        "\n",
        "        \n",
        "        # out_max = F.max_pool1d(lstm_out, kernel_size=lstm_out.shape[2])\n",
        "        # out_avg = F.avg_pool1d(lstm_out, kernel_size=lstm_out.shape[2])\n",
        "        \n",
        "        # out = torch.cat((out_avg, out_max), dim=1)\n",
        "\n",
        "        # # print(\"out  \" , out.shape)\n",
        "\n",
        "        # out = out.permute(0,2,1)\n",
        "\n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(pooled_output)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        out = sig_out.view(batch_size, -1)\n",
        "\n",
        "        out = out[:, -1]\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # initialize hidden states with sizes n_layers x batch_size x hidden_dim\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfGLNYXoFDww",
        "outputId": "1bfcbdab-7aa6-41a9-b1e1-9d0f03e384a2"
      },
      "source": [
        "no_layers = 1\n",
        "output_dim = 1\n",
        "hidden_dim = 128\n",
        "Freeze_bert = False\n",
        "model = Similarity_Model(output_dim, no_layers, hidden_dim, Freeze_bert)\n",
        "\n",
        "#moving to gpu\n",
        "model.to(device)\n",
        "\n",
        "# model_save_name = 'Snli_TechSimilarity.pt'\n",
        "# path = F\"/content/drive/My Drive/{model_save_name}\"\n",
        "# model.load_state_dict(torch.load(path))\n",
        "print(model)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity_Model(\n",
            "  (bert_model): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): BertLayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zeo_UD8FGtN"
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=2e-5\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# function to predict accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8c-N3SnFKDD"
      },
      "source": [
        "def forward_back_prop(model, optimizer, criterion, batch, hidden):\n",
        "    clip = 5\n",
        "    # input_ids, attention_masks, token_type_ids = np.array(input_ids, dtype=\"int32\"), np.array(attention_masks, dtype=\"int32\"), np.array(token_type_ids, dtype=\"int32\")\n",
        "    # move data to GPU, if available\n",
        "    input_ids, attention_masks, token_type_ids = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "    target = batch[3].to(device)\n",
        "    h = tuple([each.data for each in hidden])\n",
        "\n",
        "    optimizer.zero_grad()    \n",
        "    output, hidden = model.forward(input_ids, attention_masks, token_type_ids, h)\n",
        "\n",
        "    # calculate the loss and perform backprop\n",
        "    loss = criterion(output.squeeze(), target.float())\n",
        "    loss.backward()\n",
        "    \n",
        "    # calculating accuracy\n",
        "    accuracy = acc(output,target)\n",
        "    \n",
        "    # clip_grad_norm prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    # return the loss over a batch and the hidden state produced by our model and accuracy\n",
        "    return loss.item(), hidden, accuracy"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nhRXE2hFMp3"
      },
      "source": [
        "model_save_name = 'SentenceSimilarity.pt'\n",
        "path = F\"/content/drive/My Drive/{model_save_name}\""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkFEUcpZFQrr",
        "outputId": "cf5f5d41-d3ae-47a2-86fa-cb588993ae89"
      },
      "source": [
        "epochs = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch_i, batch in enumerate(train_dataloader, 1):\n",
        "\n",
        "        # make sure iterate over completely full batches, only\n",
        "        n_batches = len(train_dataloader.dataset)//batch_size\n",
        "        if(batch_i > n_batches):\n",
        "            break\n",
        "\n",
        "        # forward, back prop\n",
        "        loss, hidden, accuracy = forward_back_prop(model, optimizer, criterion, batch, hidden)  \n",
        "\n",
        "        train_losses.append(loss)\n",
        "        train_acc += accuracy\n",
        "        \n",
        "       \n",
        "\n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    for batch_i, (input_ids, attention_masks,token_type_ids, labels) in enumerate(validation_dataloader, 1):  \n",
        "        # make sure iterate over completely full batches, only\n",
        "        n_batches = len(validation_dataloader.dataset)//batch_size\n",
        "        if(batch_i > n_batches):\n",
        "            break\n",
        "\n",
        "        val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "        attention_masks,token_type_ids = attention_masks.to(device), token_type_ids.to(device)\n",
        "        output, val_h = model(input_ids, attention_masks, token_type_ids, val_h)\n",
        "\n",
        "        val_loss = criterion(output.squeeze(), labels.float())\n",
        "        val_losses.append(val_loss.item())\n",
        "        \n",
        "        accuracy = acc(output,labels)\n",
        "        val_acc += accuracy\n",
        "            \n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)    \n",
        "    epoch_train_acc = train_acc/len(train_dataloader.dataset)\n",
        "    epoch_val_acc = val_acc/len(validation_dataloader.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    print(f'Epoch {epoch}') \n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "    if epoch_val_loss <= valid_loss_min:\n",
        "        torch.save(model.state_dict(), path)\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
        "        valid_loss_min = epoch_val_loss\n",
        "    print(25*'==')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "train_loss : 0.32209577932953837 val_loss : 0.1939148624929098\n",
            "train_accuracy : 87.94851794071764 val_accuracy : 91.56908665105387\n",
            "Validation loss decreased (inf --> 0.193915).  Saving model ...\n",
            "==================================================\n",
            "Epoch 2\n",
            "train_loss : 0.15531486799009145 val_loss : 0.1654123473339356\n",
            "train_accuracy : 95.3978159126365 val_accuracy : 92.50585480093677\n",
            "Validation loss decreased (0.193915 --> 0.165412).  Saving model ...\n",
            "==================================================\n",
            "Epoch 3\n",
            "train_loss : 0.10013559648068622 val_loss : 0.11168482949813971\n",
            "train_accuracy : 97.11388455538221 val_accuracy : 93.91100702576112\n",
            "Validation loss decreased (0.165412 --> 0.111685).  Saving model ...\n",
            "==================================================\n",
            "Epoch 4\n",
            "train_loss : 0.06950310103129595 val_loss : 0.10491479750579366\n",
            "train_accuracy : 98.04992199687987 val_accuracy : 94.02810304449649\n",
            "Validation loss decreased (0.111685 --> 0.104915).  Saving model ...\n",
            "==================================================\n",
            "Epoch 5\n",
            "train_loss : 0.050126487243687734 val_loss : 0.07971352499981339\n",
            "train_accuracy : 98.5569422776911 val_accuracy : 94.73067915690866\n",
            "Validation loss decreased (0.104915 --> 0.079714).  Saving model ...\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDpDYiP1FS0j"
      },
      "source": [
        "# model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju-fNOyCK-rs"
      },
      "source": [
        "# predict the label given text\n",
        "def predict_similrity(text, attention, token_ids):\n",
        "        text, attention, token_ids = text.expand(1,-1), attention.expand(1,-1), token_ids.expand(1,-1)\n",
        "        # print(text.shape, attention.shape, token_ids.shape, test_dataset[i][3].shape)\n",
        "\n",
        "        inputs, attention, token_ids = text.to(device), attention.to(device), token_ids.to(device)\n",
        "        batch_size = 1\n",
        "        h = model.init_hidden(batch_size)\n",
        "        h = tuple([each.data for each in h])\n",
        "        output, h = model(inputs, attention, token_ids, h)\n",
        "        return(output.item())"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQp8Pr6mLAz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa005ce6-642c-4b29-864d-dcbf5e3ee384"
      },
      "source": [
        "#calculate test accuracy\n",
        "sum = 0.0\n",
        "for i in range(len(test_dataset)):\n",
        "        # print(test_dataset[i][0].shape, test_dataset[i][1].shape, test_dataset[i][2].shape, test_dataset[i][3].shape)\n",
        "        output = predict_similrity(test_dataset[i][0], test_dataset[i][1], test_dataset[i][2])\n",
        "        if np.round(output) == test_dataset[i][3]:\n",
        "            sum += 1\n",
        "\n",
        "accuracy = sum/len(test_dataset)\n",
        "print(f'test_accuracy : {accuracy*100}')    "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accuracy : 97.6981030539486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Ae48S5LU7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2540e6e6-2f35-4c22-8bcc-a3bc3aa2ae2c"
      },
      "source": [
        "# sentence1 = \"First, Exploding gradient and this Solved by gradient clipping. Second, Dying ReLu — No learning if the activation is 0 (Solved by parametric relu). Third, Mean and variance of activations is not 0 and 1.(Partially solved by subtracting around 0.5 from activation. Better explained in fastai videos). \"\n",
        "# sentence2 = \"ReLU units can be fragile during training and can 'die'. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold.\"\n",
        "# sentence1 = \"the neural network is a turning point\"\n",
        "# sentence2 = \"the neural network is not a turning point\"\n",
        "# sentence1=\"In a dataset, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built.\"\n",
        "# sentence2=\"We split the given data set into two different sections namely,’Training set’ and ‘Test Set’. ‘Training set’ is the portion of the dataset used to train the model. ‘Testing set’ is the portion of the dataset used to test the trained model.\"\n",
        "\n",
        "sentence1=\"A support vector machine (SVM) is a computer algorithm that learns by example to assign labels to objects\"\n",
        "sentence2=\"Support vector machines are not supervised learning algorithms used for classification and regression analysis.\"\n",
        "encoded_pair = tokenizer(  sentence1, sentence2, \n",
        "                               padding= 'max_length',  # Pad to max_length\n",
        "                               truncation= True,  # Truncate to max_length\n",
        "                               max_length= 195,  \n",
        "                               return_tensors='pt'  # Return torch.Tensor objects\n",
        "    )\n",
        "# input_ids.append(encoded_pair['input_ids'])\n",
        "# attention_masks.append(encoded_pair['attention_mask'])\n",
        "# token_type_ids.append(encoded_pair['token_type_ids'])\n",
        "\n",
        "output = predict_similrity(encoded_pair['input_ids'], encoded_pair['attention_mask'], encoded_pair['token_type_ids'])\n",
        "\n",
        "print(output)\n",
        "print(np.round(output))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.14934095740318298\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSHwyUQDuCAt"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}