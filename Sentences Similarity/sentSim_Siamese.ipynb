{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentSim_Siamese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYdUjo3syaRs"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt \n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, WeightedRandomSampler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvr1ITatz3b3",
        "outputId": "6f1aceca-2306-4b46-fb57-94b52d9b28a1"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, set our device to GPU. use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")   "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XhGsyM-I7bm"
      },
      "source": [
        "# import torchtext.vocab\n",
        "\n",
        "# glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "# print(f'There are {len(glove.itos)} words in the vocabulary')\n",
        "\n",
        "# glove.vectors.shape\n",
        "\n",
        "#glove.vectors[glove.stoi['natural']]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyM0dRGNKMnq",
        "outputId": "ae0d6e87-6cb8-43e8-d0bc-c3d4ef461d4b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D08nTcw0Eqk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "323a5444-56bd-48c6-d3dc-1e94690fb03a"
      },
      "source": [
        "# data = pd.read_csv('/content/drive/My Drive/train_snli.txt', sep=\"\\t\", header=None)\n",
        "# data.columns = [\"ans1\", \"ans2\", \"class\"]\n",
        "data = pd.read_csv('/content/drive/My Drive/Clean_train_data.csv')\n",
        "# Display 10 random rows from the data.\n",
        "data.sample(10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>ans1</th>\n",
              "      <th>ans2</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>Explain the layer of CNN: ReLU</td>\n",
              "      <td>Once a feature map is created, we can pass eac...</td>\n",
              "      <td>LSTM use persistent previous information to be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4044</th>\n",
              "      <td>What is a friend function?</td>\n",
              "      <td>operator that takes three operands: a conditio...</td>\n",
              "      <td>A friend function of a class is defined outsid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3200</th>\n",
              "      <td>What are PCA, KPCA, and ICA used for?</td>\n",
              "      <td>PCA (Principal Components Analysis) is like KP...</td>\n",
              "      <td>they are commonly used for dimensionality redu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>What is the softmax function?</td>\n",
              "      <td>RNN) are the state of the art algorithm for se...</td>\n",
              "      <td>Softmax turns arbitrary real values into proba...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>What do you understand by Tensors?</td>\n",
              "      <td>A tensor is a container which can house data i...</td>\n",
              "      <td>Tensors are mathematical objects that generali...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1268</th>\n",
              "      <td>What is the softmax function?</td>\n",
              "      <td>The Softmax regression is a form of logistic r...</td>\n",
              "      <td>The input values can be positive, negative, ze...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>What is Model Capacity?</td>\n",
              "      <td>supervised learning is where I have the input ...</td>\n",
              "      <td>The capacity of a deep learning neural network...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>631</th>\n",
              "      <td>Why is zero initialization not a good weight i...</td>\n",
              "      <td>If the set of weights in the network is put to...</td>\n",
              "      <td>in this case, the equations of the learning al...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1507</th>\n",
              "      <td>What do you understand by Tensors?</td>\n",
              "      <td>Tensors are nothing but a de facto for represe...</td>\n",
              "      <td>The Rectified Linear Unit is the most commonly...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4193</th>\n",
              "      <td>What is Software configuration management?</td>\n",
              "      <td>It is a technic of identifying, organizing, an...</td>\n",
              "      <td>Software requirements are a functional descrip...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Questions  ... class\n",
              "2221                     Explain the layer of CNN: ReLU  ...     0\n",
              "4044                         What is a friend function?  ...     0\n",
              "3200              What are PCA, KPCA, and ICA used for?  ...     1\n",
              "2389                      What is the softmax function?  ...     0\n",
              "1547                 What do you understand by Tensors?  ...     1\n",
              "1268                      What is the softmax function?  ...     1\n",
              "162                             What is Model Capacity?  ...     0\n",
              "631   Why is zero initialization not a good weight i...  ...     1\n",
              "1507                 What do you understand by Tensors?  ...     0\n",
              "4193         What is Software configuration management?  ...     0\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HbJO7ck_tWD"
      },
      "source": [
        "def clean_text(text):\n",
        "    #Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean2_text(text):\n",
        "  #remove some of stopwords as 'a, an, the', ... \n",
        "  txt = clean_text(text)\n",
        "  words = txt.split(' ')\n",
        "  aft_remove = [w for w in words if w not in ['a', 'an', 'the', 'of', 'that', 'which','in', 'at', 'for', 'where', 'when', 'to', 'from']]\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "# cl_tx = clean2_text(p)\n",
        "# print(cl_tx)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-1QfUJRLAur",
        "outputId": "46df7695-2269-4247-8ce5-7261ec861b9a"
      },
      "source": [
        "#Convert train data into list of tuples where each tuple is of the form (sentence1, sentence2)\n",
        "sentences_pair = []\n",
        "labels = []\n",
        "for _, row in data.iterrows():\n",
        "    \n",
        "    s1 = clean2_text(str(row['ans1']))\n",
        "    s2 = clean2_text(str(row['ans2']))\n",
        "    label = int(row['class'])\n",
        "    \n",
        "    if s1 and s2:\n",
        "        sentences_pair.append(( s1, s2 ))\n",
        "        labels.append(label)\n",
        "\n",
        "print ('Train Data sentences Pairs: ', len(sentences_pair))\n",
        "print(sentences_pair[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data sentences Pairs:  4274\n",
            "('deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain which is called an artificial neural network deep learning is suited over a range of fields such as computer vision speech recognition natural language processing etc ', 'deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled also known as deep neural learning or deep neural network ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0YwVimLdkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c04ba1a-d0fb-4d32-d147-4416c1c879e3"
      },
      "source": [
        "Vocab = {}\n",
        "n_words = 0\n",
        "\n",
        "#build vocab using training data\n",
        "for data in [sentences_pair]:\n",
        "  for sentence_pair in data:\n",
        "      s1 = sentence_pair[0]\n",
        "      s2 = sentence_pair[1]\n",
        "      for word in s1.split(' '):\n",
        "        if word not in Vocab:\n",
        "          Vocab[word] = n_words + 1\n",
        "          n_words += 1\n",
        "      \n",
        "      for word in s2.split(' '):\n",
        "        if word not in Vocab:\n",
        "          Vocab[word] = n_words + 1\n",
        "          n_words += 1\n",
        "    \n",
        "    \n",
        "print(\"Total words in vocab are\",len(Vocab))\n",
        "# print(Vocab)  "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in vocab are 2560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LWqwjs8Mk5c"
      },
      "source": [
        "class SentencesDataset(Dataset):\n",
        "    def __init__(self, sentences_list, word2index, labels):\n",
        "        self.sentences_list = sentences_list\n",
        "        self.labels = labels\n",
        "        self.word2index = word2index\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sentences_list)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        sentences_pair = self.sentences_list[index]\n",
        "        s1 = sentences_pair[0]\n",
        "        s1_indices = []\n",
        "        for word in s1.split():\n",
        "            s1_indices.append(self.word2index[word])\n",
        "            \n",
        "        s2 = sentences_pair[1]\n",
        "        s2_indices = []\n",
        "        for word in s2.split():\n",
        "            s2_indices.append(self.word2index[word])\n",
        "            \n",
        "        # s1_indices and s2_indices are lists of indices against words used in the sentence \n",
        "        return s1_indices, s2_indices, self.labels[index]\n",
        "    \n",
        "train_dataset = SentencesDataset(sentences_pair, Vocab, labels)\n",
        "# print(len(train_dataset))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boZZNKFcNMBR"
      },
      "source": [
        "class CustomCollate:\n",
        "    def custom_collate(self, batch):\n",
        "\n",
        "        # batch = list of tuples where each tuple is of the form ([i1, i2, i3], [j1, j2, j3], label)\n",
        "        s1_list = []\n",
        "        s2_list = []\n",
        "        labels = []\n",
        "        for training_example in batch:\n",
        "          s1_list.append(training_example[0])\n",
        "          s2_list.append(training_example[1])\n",
        "          labels.append(training_example[2])\n",
        "          \n",
        "        \n",
        "        s1_lengths = [len(s) for s in s1_list]\n",
        "        s2_lengths = [len(s) for s in s2_list]\n",
        "        \n",
        "        return s1_list, s1_lengths, s2_list, s2_lengths, labels\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self.custom_collate(batch)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGjicwKmN107",
        "outputId": "8e0f72b5-cc9b-4742-c539-52679526b704"
      },
      "source": [
        "validation_split = 0.1\n",
        "dataset_size = len(train_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "shuffle_dataset = True\n",
        "random_seed = 32\n",
        "\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "validation_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=CustomCollate())\n",
        "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=validation_sampler, collate_fn=CustomCollate())\n",
        "\n",
        "print ('Training Set Size {}, Validation Set Size {}'.format(len(train_indices), len(val_indices)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Size 3847, Validation Set Size 427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLRM3SRnO-aW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46121f77-4845-443f-b8ff-1bed09abed2b"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "702LvIjMpjqW",
        "outputId": "d736d323-634f-4e93-d250-f68f7e3cb5aa"
      },
      "source": [
        "vec_king = wv['king']\n",
        "wv.vectors"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.1291504e-03, -8.9645386e-04,  3.1852722e-04, ...,\n",
              "        -1.5640259e-03, -1.2302399e-04, -8.6307526e-05],\n",
              "       [ 7.0312500e-02,  8.6914062e-02,  8.7890625e-02, ...,\n",
              "        -4.7607422e-02,  1.4465332e-02, -6.2500000e-02],\n",
              "       [-1.1779785e-02, -4.7363281e-02,  4.4677734e-02, ...,\n",
              "         7.1289062e-02, -3.4912109e-02,  2.4169922e-02],\n",
              "       ...,\n",
              "       [-1.9653320e-02, -9.0820312e-02, -1.9409180e-02, ...,\n",
              "        -1.6357422e-02, -1.3427734e-02,  4.6630859e-02],\n",
              "       [ 3.2714844e-02, -3.2226562e-02,  3.6132812e-02, ...,\n",
              "        -8.8500977e-03,  2.6977539e-02,  1.9042969e-02],\n",
              "       [ 4.5166016e-02, -4.5166016e-02, -3.9367676e-03, ...,\n",
              "         7.9589844e-02,  7.2265625e-02,  1.3000488e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQEFyEMpOFde"
      },
      "source": [
        "# Convert word2vec embeddings into FloatTensor\n",
        "word2vec_model = wv\n",
        "word2vec_weights = torch.FloatTensor(word2vec_model.vectors)\n",
        "embeddingSize = 300\n",
        "\n",
        "weights = torch.randn(n_vocabulary_words+1, embeddingSize)\n",
        "weights[0] = torch.zeros(embeddingSize)\n",
        "for word, lang_word_index in Vocab.items():\n",
        "    if word in word2vec_model:\n",
        "        weights[lang_word_index] = torch.FloatTensor(word2vec_model.word_vec(word))\n",
        "\n",
        "# del word2vec_model\n",
        "del word2vec_weights"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47LkE_ZCv0fp"
      },
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, pretrained_weights, hidden_dim, embeddingSize, no_layers):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embeddingSize = embeddingSize\n",
        "        self.no_layers = no_layers\n",
        "\n",
        "        # Embedding layers from the pre-trained weights\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_weights)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        # Create a single LSTM since this is a Siamese Network and the weights are shared\n",
        "        self.lstm = nn.LSTM(input_size=embeddingSize, hidden_size=hidden_dim, num_layers = no_layers, batch_first = True)\n",
        "    \n",
        "    # Manhattan Distance Calculator\n",
        "    def exponent_neg_manhattan_distance(self, x1, x2):\n",
        "        return torch.exp(-torch.sum(torch.abs(x1 - x2), dim=0)).to(device)\n",
        "\n",
        "    def forward_once(self, x, input_lengths, hidden):\n",
        "      \n",
        "        # Reverse sequence lengths indices in decreasing order before Padding and Packing\n",
        "        sorted_indices = np.flipud(np.argsort(input_lengths))\n",
        "        input_lengths = np.flipud(np.sort(input_lengths))\n",
        "        input_lengths = input_lengths.copy() \n",
        "        \n",
        "        # Reorder questions in the decreasing order of their lengths\n",
        "        ordered_sentences = [torch.LongTensor(x[i]).to(device) for i in sorted_indices]\n",
        "        # Pad sequences with 0s to the max length sequence in the batch\n",
        "        ordered_sentences = torch.nn.utils.rnn.pad_sequence(ordered_sentences, batch_first=True)\n",
        "\n",
        "      \n",
        "        embeddings = self.embedding(ordered_sentences).to(device)\n",
        "        \n",
        "        # Pack the padded sequences and pass it through LSTM\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeddings, input_lengths, batch_first=True)\n",
        "        out, hidden = self.lstm(packed, hidden)\n",
        "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=int(input_lengths[0]))\n",
        "        \n",
        "        # The following step reorders the calculated activations to the original order in which sentences were passed\n",
        "        result = torch.FloatTensor(unpacked.size())\n",
        "        for i, encoded_matrix in enumerate(unpacked):\n",
        "            result[sorted_indices[i]] = encoded_matrix\n",
        "        return result\n",
        "\n",
        "    def forward(self, s1, s1_lengths, s2, s2_lengths, hidden):\n",
        "        output1 = self.forward_once(s1, s1_lengths, hidden)  #shape of output1 (batch_size, sequence length, hidden_dim)\n",
        "        output2 = self.forward_once(s2, s2_lengths, hidden)  #shape of output2 (batch_size, sequence length, hidden_dim)\n",
        "\n",
        "        # Calculate Similarity Score between both sentences in a single pair\n",
        "        similarity_score = torch.zeros(output1.size()[0]).to(device)\n",
        "        for index in range(output1.size()[0]):\n",
        "            s1 = output1[index, s1_lengths[index] - 1, :]\n",
        "            s2 = output2[index, s2_lengths[index] - 1, :]\n",
        "            similarity_score[index] = self.exponent_neg_manhattan_distance(s1, s2)\n",
        "        return similarity_score\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # initialize hidden states with sizes n_layers x batch_size x hidden_dim\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden    \n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrT5akUuuI6u",
        "outputId": "cbb7df92-0bcb-402b-d169-3a8bb296c0c2"
      },
      "source": [
        "no_layers = 1\n",
        "embeddingSize = 300\n",
        "hidden_dim = 128\n",
        "\n",
        "model = SiameseNetwork(weights, hidden_dim, embeddingSize, no_layers)\n",
        "#moving to gpu\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SiameseNetwork(\n",
            "  (embedding): Embedding(2752, 300)\n",
            "  (lstm): LSTM(300, 128, batch_first=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEFlC_uwxIFS"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01 )\n",
        "\n",
        "# function to predict accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFKRJocQxl5Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baed0983-fabb-412d-f4f6-6a80363e35c2"
      },
      "source": [
        "# Threshold 0.5. Since similarity score will be a value between 0 and 1, we will consider all sentences pair with values greater than threshold as the same meaning\n",
        "num_epochs = 5\n",
        "clip = 5\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    # initialize hidden state\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch_i, (s1_batch, s1_batch_lengths, s2_batch, s2_batch_lengths, labels) in enumerate(train_loader, 1):\n",
        "\n",
        "        n_batches = len(train_indices)//batch_size\n",
        "        if(batch_i > n_batches):\n",
        "            break\n",
        "\n",
        "        labels = torch.FloatTensor(labels).to(device)\n",
        "        h = tuple([each.data for each in hidden])\n",
        "        \n",
        "        # Clear grads\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Run the forward pass\n",
        "        similarity_score = model(s1_batch, s1_batch_lengths, s2_batch, s2_batch_lengths, hidden)\n",
        "\n",
        "        # Calculate Loss\n",
        "        loss = criterion(similarity_score, labels)\n",
        "        # Calculate gradients\n",
        "        loss.backward()\n",
        "                \n",
        "        # clip_grad_norm prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculating accuracy\n",
        "        accuracy = acc(similarity_score,labels)\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        train_acc += accuracy\n",
        "        \n",
        "            \n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    # with torch.no_grad():\n",
        "    for batch_i, (s1_batch, s1_batch_lengths, s2_batch, s2_batch_lengths, labels) in enumerate(val_loader, 1):\n",
        "        # make sure iterate over completely full batches, only\n",
        "        n_batches = len(val_indices)//batch_size\n",
        "        if(batch_i > n_batches):\n",
        "            break\n",
        "\n",
        "        labels = torch.FloatTensor(labels).to(device)\n",
        "\n",
        "        similarity_score = model(s1_batch, s1_batch_lengths, s2_batch, s2_batch_lengths, val_h)\n",
        "        \n",
        "        val_loss = criterion(similarity_score, labels)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        accuracy = acc(similarity_score,labels)\n",
        "        val_acc += accuracy\n",
        "        \n",
        "           \n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)  \n",
        "\n",
        "    epoch_train_acc = train_acc/len(train_indices)\n",
        "    epoch_val_acc = val_acc/len(val_indices)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    print(f'Epoch {epoch}') \n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "    if epoch_val_loss <= valid_loss_min:\n",
        "        # torch.save(model.state_dict(), path)\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
        "        valid_loss_min = epoch_val_loss\n",
        "    print(25*'==')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "train_loss : 0.2265402643630902 val_loss : 0.21481963189748618\n",
            "train_accuracy : 69.30075383415648 val_accuracy : 67.68149882903981\n",
            "Validation loss decreased (inf --> 0.214820).  Saving model ...\n",
            "==================================================\n",
            "Epoch 1\n",
            "train_loss : 0.17901735939085484 val_loss : 0.19744295111069313\n",
            "train_accuracy : 73.61580452300494 val_accuracy : 68.38407494145198\n",
            "Validation loss decreased (0.214820 --> 0.197443).  Saving model ...\n",
            "==================================================\n",
            "Epoch 2\n",
            "train_loss : 0.15130097679793836 val_loss : 0.17829346542175\n",
            "train_accuracy : 79.02261502469456 val_accuracy : 71.19437939110071\n",
            "Validation loss decreased (0.197443 --> 0.178293).  Saving model ...\n",
            "==================================================\n",
            "Epoch 3\n",
            "train_loss : 0.13099400339027245 val_loss : 0.16336578417282838\n",
            "train_accuracy : 83.18170002599427 val_accuracy : 74.00468384074942\n",
            "Validation loss decreased (0.178293 --> 0.163366).  Saving model ...\n",
            "==================================================\n",
            "Epoch 4\n",
            "train_loss : 0.11378925588602821 val_loss : 0.15246669833476728\n",
            "train_accuracy : 87.15882505848714 val_accuracy : 78.22014051522248\n",
            "Validation loss decreased (0.163366 --> 0.152467).  Saving model ...\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_DEtowr8egd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}