{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51137d49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51137d49",
    "outputId": "675f30e6-b50c-4b01-8d6c-88192a2133f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: face-alignment in /home/ahmed/anaconda3/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: numba in /home/ahmed/anaconda3/lib/python3.8/site-packages (from face-alignment) (0.53.1)\n",
      "Requirement already satisfied: torch in /home/ahmed/anaconda3/lib/python3.8/site-packages (from face-alignment) (1.8.1)\n",
      "Requirement already satisfied: scikit-image in /home/ahmed/anaconda3/lib/python3.8/site-packages (from face-alignment) (0.18.1)\n",
      "Requirement already satisfied: tqdm in /home/ahmed/anaconda3/lib/python3.8/site-packages (from face-alignment) (4.59.0)\n",
      "Requirement already satisfied: scipy>=0.17 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from face-alignment) (1.6.2)\n",
      "Requirement already satisfied: numpy in /home/ahmed/anaconda3/lib/python3.8/site-packages (from face-alignment) (1.20.1)\n",
      "Requirement already satisfied: opencv-python in /home/ahmed/anaconda3/lib/python3.8/site-packages (from face-alignment) (4.5.2.54)\n",
      "Requirement already satisfied: setuptools in /home/ahmed/anaconda3/lib/python3.8/site-packages (from numba->face-alignment) (52.0.0.post20210125)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from numba->face-alignment) (0.36.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from scikit-image->face-alignment) (3.3.4)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from scikit-image->face-alignment) (2.5)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from scikit-image->face-alignment) (8.2.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from scikit-image->face-alignment) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from scikit-image->face-alignment) (2020.10.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from scikit-image->face-alignment) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (1.3.1)\n",
      "Requirement already satisfied: six in /home/ahmed/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (1.15.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ahmed/anaconda3/lib/python3.8/site-packages (from networkx>=2.0->scikit-image->face-alignment) (5.0.6)\n",
      "Requirement already satisfied: typing_extensions in /home/ahmed/anaconda3/lib/python3.8/site-packages (from torch->face-alignment) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install face-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "QZRmCXyY55kk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZRmCXyY55kk",
    "outputId": "da02398c-7aee-46fc-bab1-70a0f98100c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /home/ahmed/anaconda3/lib/python3.8/site-packages (0.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f7c43d",
   "metadata": {
    "id": "11f7c43d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1e7c0261bd17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubsetRandomSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFaceAligner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJavascript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import face_alignment\n",
    "from skimage import io,transform,data,color,feature\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import skimage.io as io\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score , GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from numba import jit, cuda\n",
    "import pickle\n",
    "import timeit\n",
    "from os import listdir\n",
    "from matplotlib import image\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import string\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dlib\n",
    "from imutils.face_utils import FaceAligner\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481d410",
   "metadata": {
    "id": "8481d410"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import glob\n",
    "import shutil\n",
    "import wave\n",
    "import contextlib\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2c7d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4be2c7d4",
    "outputId": "6925c9ca-4bb1-47a8-b3fd-77b767b82560"
   },
   "outputs": [],
   "source": [
    "# put here the path to the mode\n",
    "model = pickle.load(open(\"faceDetection2.sav\", 'rb'))\n",
    "print(model)\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d650428",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d650428",
    "outputId": "6fcfaadf-e5c0-4744-d4c6-d406578b0c7f"
   },
   "outputs": [],
   "source": [
    "#load the model \n",
    "voice_model = pickle.load(open('voice_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5499f54",
   "metadata": {
    "id": "d5499f54"
   },
   "outputs": [],
   "source": [
    "def getFaceAlign(img):\n",
    "  #ios.imsave('testt.jpg',img)\n",
    "  preds = fa.get_landmarks(img)\n",
    "  if preds == None:\n",
    "    #print('Warning: No faces were detected.')\n",
    "    return None,0\n",
    "  if preds!=None:\n",
    "    mnXY= np.min(np.min(preds, axis=1), axis=0)\n",
    "    mxXY= np.max(np.max(preds, axis=1), axis=0)\n",
    "    mxY=int(mxXY[0]+2)\n",
    "    mnX=int(mnXY[1])-8\n",
    "    mxX=int(mxXY[1])\n",
    "    if mnX < 0:\n",
    "       mnX=0\n",
    "    if int(mxXY[0]+2) >= img.shape[1]:\n",
    "      mxY=img.shape[1]-1\n",
    "    if (img.shape[0]-2-mnX) <= 40:\n",
    "      return img[mnX:img.shape[0]-2,int(mnXY[0])-2:mxY],1\n",
    "    else:\n",
    "      return img[mnX:mnX+40,int(mnXY[0])-2:mxY],1   # were 40 instead of 50\n",
    "    ####################################\n",
    "    # im_pil = Image.fromarray(img[mnX:mxX,int(mnXY[0]):mxY])\n",
    "    # im_pil=im_pil.resize((100, 100), Image.ANTIALIAS)\n",
    "    # io.imshow(np.array(im_pil))\n",
    "    # io.imsave('testtt.jpg',np.array(im_pil))\n",
    "    # return np.array(im_pil),1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a7d03",
   "metadata": {
    "id": "ba7a7d03"
   },
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "    pick = []\n",
    "    x1 = boxes[:,2]\n",
    "    y1 = boxes[:,0]\n",
    "    x2 = boxes[:,3]\n",
    "    y2 = boxes[:,1]\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "    return boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec24f65",
   "metadata": {
    "id": "7ec24f65"
   },
   "outputs": [],
   "source": [
    "def find(image):\n",
    "    boxes=[[i,i+k,j,int(j+k)] for k in [35,45,55] for i in range(0,image.shape[0]-k,5) for j in range(0,image.shape[1]-k,5) if (model.predict([feature.hog(transform.resize(image[i:i+k,j:int(j+k)],(48,48)))])[0] == 1)]\n",
    "    #boxes=[[i,i+k,j,int(j+k)] for k in [35,40,45,50] for i in range(0,image.shape[0]-k,10) for j in range(0,image.shape[1]-k,10) if (model.predict([feature.hog(transform.resize(image[i:i+k,j:int(j+k)],(48,48)))])[0] == 1)]\n",
    "    #boxes=[[i,i+k,j,int(j+k)] for k in [150,160,170,180,250,260,270,280,350,360,370,400,420,450] for i in range(0,image.shape[0]-k,50) for j in range(0,image.shape[1]-k,50) if (model.predict([feature.hog(transform.resize(image[i:i+k,j:int(j+k)],(48,48)))])[0] == 1)]\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9476384",
   "metadata": {
    "id": "e9476384"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from torch.nn.modules.activation import ReLU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def SeparableConv2D(in_channels, out_channels, kernel=3):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, in_channels, kernel_size=kernel, stride=1, groups=in_channels,padding=1, bias=False),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "    )\n",
    "\n",
    "class ResidualXceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel=3):\n",
    "        super(ResidualXceptionBlock, self).__init__()\n",
    "        global device\n",
    "\n",
    "        self.depthwise_conv1 = SeparableConv2D(in_channels, out_channels, kernel).to(device)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.depthwise_conv2 = SeparableConv2D(out_channels, out_channels, kernel).to(device)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # self.padd = nn.ZeroPad2d(22)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, padding=22, bias=False)\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.residual_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # residual branch\n",
    "        residual = self.residual_conv(x)\n",
    "        residual = self.residual_bn(residual)\n",
    "        \n",
    "        # print('input',x.shape)\n",
    "        # feature extraction branch\n",
    "        x = self.depthwise_conv1(x)\n",
    "        # print('conv1',x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.depthwise_conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        # print('conv2',x.shape)\n",
    "\n",
    "        # x = self.padd(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x[:,:, 11:22, 11:22])\n",
    "        # print('max_pooling',x.shape)\n",
    "        # print('res',residual.shape)\n",
    "        return x + residual\n",
    "\n",
    "class Mini_Xception(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mini_Xception, self).__init__()\n",
    "        self.conv1 = conv_bn_relu(1, 8, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = conv_bn_relu(8, 8, kernel_size=3, stride=1, padding=0)\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            ResidualXceptionBlock(8 , 16).to(device),\n",
    "            ResidualXceptionBlock(16, 32).to(device),\n",
    "            ResidualXceptionBlock(32, 64).to(device),\n",
    "            ResidualXceptionBlock(64, 128).to(device)            \n",
    "        ])\n",
    "        self.conv3 = nn.Conv2d(128, 7, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "            # print('ith block', x.shape, block.device)\n",
    "\n",
    "        # print('blocks:',x.shape)\n",
    "        x = self.conv3(x)\n",
    "        # print('conv3',x.shape)\n",
    "        x = self.global_avg_pool(x)\n",
    "        # # x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_label_emotion(label : int) -> str:\n",
    "    label_emotion_map = { \n",
    "        0: 'Angry',\n",
    "        1: 'Disgust', \n",
    "        2: 'Fear', \n",
    "        3: 'Happy', \n",
    "        4: 'Sad', \n",
    "        5: 'Surprise', \n",
    "        6: 'Neutral'        \n",
    "    }\n",
    "    return label_emotion_map[label]\n",
    "\n",
    "\n",
    "def pr(img):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  mini_xception = Mini_Xception().to(device)\n",
    "  mini_xception.eval()\n",
    "\n",
    "  # Load model\n",
    "  # put here the path to the model\n",
    "  checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/weights_epoch_75.pth.tar')\n",
    "  mini_xception.load_state_dict(checkpoint['mini_xception'])\n",
    "\n",
    "    \n",
    "  input_face = cv2.resize(img, (48,48))\n",
    "  input_face = cv2.equalizeHist(input_face)\n",
    "  input_face = transforms.ToTensor()(input_face).to(device)\n",
    "  input_face = torch.unsqueeze(input_face, 0)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      input_face = input_face.to(device)\n",
    "      emotion = mini_xception(input_face)\n",
    "      # print(f'\\ntime={(time.time()-t) * 1000 } ms')\n",
    "\n",
    "      torch.set_printoptions(precision=6)\n",
    "      softmax = torch.nn.Softmax()\n",
    "      emotions_soft = softmax(emotion.squeeze()).reshape(-1,1).cpu().detach().numpy()\n",
    "      emotions_soft = np.round(emotions_soft, 3)\n",
    "      for i, em in enumerate(emotions_soft):\n",
    "          em = round(em.item(),3)\n",
    "          # print(f'{get_label_emotion(i)} : {em}')\n",
    "\n",
    "      emotion = torch.argmax(emotion)                \n",
    "      percentage = round(emotions_soft[emotion].item(), 2)\n",
    "      emotion = emotion.squeeze().cpu().detach().item()\n",
    "      emotion = get_label_emotion(emotion)\n",
    "      return emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2921901",
   "metadata": {
    "id": "c2921901"
   },
   "source": [
    "# Voice analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c0aeb",
   "metadata": {
    "id": "e98c0aeb"
   },
   "outputs": [],
   "source": [
    "#DataFlair - Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(X, mfcc, chroma, mel,sample_rate):   \n",
    "    result=np.array([])\n",
    "    if mfcc:\n",
    "        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        stft=np.abs(librosa.stft(X))\n",
    "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa92cd3",
   "metadata": {
    "id": "eaa92cd3"
   },
   "outputs": [],
   "source": [
    "def predict_voice(file):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    y, s = librosa.load(file) # Downsample 44.1kHz to 8kHz\n",
    "    feature=extract_feature(X=y, mfcc=True, chroma=True, mel=True,sample_rate=s)\n",
    "    x.append(feature)\n",
    "    y_pred=voice_model.predict(x)\n",
    "    emotion = y_pred[0]\n",
    "    return emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6QphZfZz6aiT",
   "metadata": {
    "id": "6QphZfZz6aiT"
   },
   "source": [
    "## Split Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sHDxh4ne6edI",
   "metadata": {
    "id": "sHDxh4ne6edI"
   },
   "outputs": [],
   "source": [
    "def get_duration(fname):\n",
    "    with contextlib.closing(wave.open(fname,'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "        return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JQtWZYvH6g77",
   "metadata": {
    "id": "JQtWZYvH6g77"
   },
   "outputs": [],
   "source": [
    "def get_emotion_dic(fname):\n",
    "  duration = get_duration(fname) \n",
    "  \n",
    "  # frames/ is the directory to stores audio frames\n",
    "  dir='frames'\n",
    "  if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "  else:\n",
    "    shutil.rmtree(dir)           \n",
    "    os.makedirs(dir)  \n",
    "  emotion_dic={'bad':0,'medium':0,'good':0}\n",
    "  originalAudio = AudioSegment.from_wav(fname)\n",
    "  for i in range(0,int(duration),3):\n",
    "      #newAudio = AudioSegment.from_wav(\"ted2.wav\")\n",
    "      newAudio = originalAudio[i*1000:(i+3)*1000]\n",
    "      newAudio.export('frames/'+str(i)+'.wav', format=\"wav\") #Exports to a wav file in the current path.\n",
    "      emotion = predict_voice('frames/'+str(i)+'.wav') \n",
    "      emotion_dic[emotion]+=1\n",
    "  return emotion_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3nuLkGRrIKrr",
   "metadata": {
    "id": "3nuLkGRrIKrr"
   },
   "source": [
    "# **Answers Similarity**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rP7WA-htIXVw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rP7WA-htIXVw",
    "outputId": "52ee2839-dbc6-452c-e7cb-c8e478b35bf7"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import AutoTokenizer\n",
    "!pip install pytorch-pretrained-bert\n",
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaw5DZ5Ic6l",
   "metadata": {
    "id": "4eaw5DZ5Ic6l"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def clean2_text(text):\n",
    "  #remove some of stopwords as 'a, an, the'\n",
    "  txt = clean_text(text)\n",
    "  words = txt.split(' ')\n",
    "  aft_remove = [w for w in words if w not in ['a', 'an', 'the','of', 'that', '']]\n",
    "  return ' '.join(aft_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UCdeXPhUIfVf",
   "metadata": {
    "id": "UCdeXPhUIfVf"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K7g8AzT8Ih9M",
   "metadata": {
    "id": "K7g8AzT8Ih9M"
   },
   "outputs": [],
   "source": [
    "class Similarity_Model(nn.Module):\n",
    "    def __init__(self, output_dim, n_layers, hidden_dim, freeze_bert):\n",
    "        super(Similarity_Model,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        \n",
    "        #bert Model and Freeze the BERT model\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        #LSTM layers\n",
    "        # self.lstm = nn.LSTM(768, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear layer\n",
    "        self.fc = nn.Linear(768, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, token_type_ids, hidden):\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        sequence_output, pooled_output = self.bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids) \n",
    "        # print(\"seq  \" , len(sequence_output))\n",
    "        \n",
    "        # lstm_out, hidden = self.lstm(sequence_output[0], hidden)\n",
    "\n",
    "        # lstm_out = lstm_out.permute(0,2,1)\n",
    "        \n",
    "        # out_max = F.max_pool1d(lstm_out, kernel_size=lstm_out.shape[2])\n",
    "        # out_avg = F.avg_pool1d(lstm_out, kernel_size=lstm_out.shape[2])\n",
    "        \n",
    "        # out = torch.cat((out_avg, out_max), dim=1)\n",
    "        # out = out.permute(0,2,1)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(pooled_output)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        out = out[:, -1]\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # initialize hidden states with sizes n_layers x batch_size x hidden_dim\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1JWsg5ARnxmT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1JWsg5ARnxmT",
    "outputId": "eefa6e8d-c709-4c8e-d5de-1d24999d3e5f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qczUheufIjBS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qczUheufIjBS",
    "outputId": "4633dd48-1915-4f7b-b8d0-7295d8ad2697"
   },
   "outputs": [],
   "source": [
    "no_layers = 1\n",
    "output_dim = 1\n",
    "hidden_dim = 128\n",
    "Freeze_bert = False\n",
    "Similar_Model = Similarity_Model(output_dim, no_layers, hidden_dim, Freeze_bert)\n",
    "#moving to gpu\n",
    "Similar_Model.to(device)\n",
    "Similar_Model.eval()\n",
    "\n",
    "Similar_Model.load_state_dict(torch.load(\"/content/drive/MyDrive/Snli_TechSimilarity.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsvETCozIpK9",
   "metadata": {
    "id": "tsvETCozIpK9"
   },
   "outputs": [],
   "source": [
    "def predict_similarity(ans1, ans2):\n",
    "  sentence1 = clean2_text(ans1)\n",
    "  sentence2 = clean2_text(ans2)\n",
    "  print(sentence1, sentence2)\n",
    "  encoded_pair = tokenizer(  sentence1, sentence2, \n",
    "      add_special_tokens=True,\n",
    "      return_tensors='pt'  # Return torch.Tensor objects\n",
    "  )\n",
    "  text, attention, token_ids = encoded_pair['input_ids'].expand(1,-1), encoded_pair['attention_mask'].expand(1,-1), encoded_pair['token_type_ids'].expand(1,-1)\n",
    "  inputs, attention, token_ids = text.to(device), attention.to(device), token_ids.to(device)\n",
    "  h = Similar_Model.init_hidden(1)\n",
    "  h = tuple([each.data for each in h])\n",
    "  output, _ = Similar_Model(inputs, attention, token_ids, h)\n",
    "  return str(output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LQCdQbv_Z5Ih",
   "metadata": {
    "id": "LQCdQbv_Z5Ih"
   },
   "source": [
    "# **Speech Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UUmRNecnaA9h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUmRNecnaA9h",
    "outputId": "1ef8c1b8-f766-4f74-e781-dc197061e11c"
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud\n",
    "!pip install google-cloud-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uFP5ppS8aFEn",
   "metadata": {
    "id": "uFP5ppS8aFEn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import io as ar\n",
    "import os\n",
    "from google.cloud import speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z62TAS94aQ3Z",
   "metadata": {
    "id": "Z62TAS94aQ3Z"
   },
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'avian-tract-283207-f1553ac44767.json'\n",
    "client = speech.SpeechClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4nvfNhB2amE3",
   "metadata": {
    "id": "4nvfNhB2amE3"
   },
   "outputs": [],
   "source": [
    "def SpeechRecognition(audiof):\n",
    "  x, s = librosa.load(audiof)\n",
    "  soundfile.write('tmp.wav', x, s)\n",
    "  with ar.open('/content/tmp.wav','rb') as audio_file:\n",
    "    content = audio_file.read()\n",
    "  audio = speech.RecognitionAudio(content=content)\n",
    "  config = speech.RecognitionConfig(\n",
    "      encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "      language_code = 'en-US')\n",
    "  operation = client.long_running_recognize(config=config, audio=audio)\n",
    "  print(\"Waiting for operation to complete...\")\n",
    "  response = operation.result(timeout=90)\n",
    "  answer = \"\"\n",
    "  for result in response.results:\n",
    "      # print('Transcript: {}'.format(result.alternatives[0].transcript))\n",
    "      answer += result.alternatives[0].transcript\n",
    "  return answer    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1cBO8pnIvBp",
   "metadata": {
    "id": "p1cBO8pnIvBp"
   },
   "source": [
    "# **Routes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720c670",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8720c670",
    "outputId": "a30bb326-1c46-4ab8-c117-4fb473d4980a"
   },
   "outputs": [],
   "source": [
    "!pip install flask-ngrok\n",
    "!pip install -U flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50beadcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50beadcd",
    "outputId": "aa353bf7-4502-4821-f539-5f77f87760fe"
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "#this contains the path of folder that to store the images in\n",
    "app.config[\"IMAGE_UPLOADS\"] = \"/content\"\n",
    "app.config[\"AUDIO_UPLOADS\"]=\"/content\"\n",
    "run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "#@app.route('/') \n",
    "#def home():\n",
    "#    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/predict\", methods=['post'])\n",
    "def predict():\n",
    "    image_bytes = b64decode(request.form[\"image\"].split(',')[1])\n",
    "    # convert bytes to numpy array\n",
    "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "    # decode numpy array into OpenCV BGR image\n",
    "    image = cv2.imdecode(jpg_as_np, flags=1)\n",
    "\n",
    "    url = request.method\n",
    "    frameTemp = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    frameTemp = cv2.resize(frameTemp, (100, 100))\n",
    "    try:\n",
    "      rects = find(frameTemp)\n",
    "      rects = np.array(rects)\n",
    "      rects = non_max_suppression_fast(rects, 0.2)\n",
    "\n",
    "      output=\"\"\n",
    "      ans = \"\"\n",
    "      for rect in rects: \n",
    "          res,ret=getFaceAlign(frameTemp[rect[0]:rect[1],rect[2]:rect[3]])\n",
    "          if ret==1:\n",
    "            pred = pr(res)\n",
    "            print(pred)\n",
    "            if pred in ['Disgust','Fear','Sad','Angry']:\n",
    "              ans = 'bad'\n",
    "            elif pred in ['Surprise' , 'Neutral']:\n",
    "              ans = 'medium'\n",
    "            elif pred == 'Happy':\n",
    "              ans = 'good'             \n",
    "            output+= ans+\" \"\n",
    "            print(ans)\n",
    "      #return the emotion of the face's image to the html\n",
    "      if output!=\"\":\n",
    "        return jsonify(output)\n",
    "      return jsonify(\"medium\")\n",
    "    \n",
    "    except:\n",
    "        return jsonify(\"medium\")\n",
    "\n",
    "@app.route('/predictVoice',methods=['POST'])\n",
    "def predictVoice():\n",
    "    audio = request.files[\"file\"]\n",
    "    audiofile=audio.filename\n",
    "    #save the audio in colab directory /content \n",
    "    audio.save(os.path.join(app.config[\"AUDIO_UPLOADS\"], audiofile))\n",
    "    #dir of audio is in the current directory \n",
    "    #convert audio to correct wav file\n",
    "    newAudio = AudioSegment.from_file(audiofile)\n",
    "    #overwrite corrupted audio with the corrected audio \n",
    "    newAudio.export(audiofile, format=\"wav\")\n",
    "    emotion_dic=get_emotion_dic(\"/content/\"+audiofile)\n",
    "    return emotion_dic\n",
    "\n",
    "@app.route('/predictSimilarity', methods=['POST'])\n",
    "def predictSimilarity():\n",
    "    audio = request.files[\"file\"]\n",
    "    audiofile=audio.filename\n",
    "    audio.save(os.path.join(app.config[\"AUDIO_UPLOADS\"], audio.filename))\n",
    "    # #convert audio to correct wav file\n",
    "    # newAudio = AudioSegment.from_file(audiofile)\n",
    "    # #overwrite corrupted audio with the corrected audio \n",
    "    # newAudio.export(audiofile, format=\"wav\")\n",
    "\n",
    "    answer1 = request.form['ans1']\n",
    "    print(answer1)\n",
    "\n",
    "    answer2 = SpeechRecognition(\"/content/\"+audiofile)\n",
    "    print(answer2)\n",
    "    prob = predict_similarity(answer1, answer2)\n",
    "    return prob    \n",
    "\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deploy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
