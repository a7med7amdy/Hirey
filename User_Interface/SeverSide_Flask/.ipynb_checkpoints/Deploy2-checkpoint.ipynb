{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51137d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install face-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import face_alignment\n",
    "from skimage import io,transform,data,color,feature\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import skimage.io as io\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score , GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from numba import jit, cuda\n",
    "import pickle\n",
    "import timeit\n",
    "from os import listdir\n",
    "from matplotlib import image\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dlib\n",
    "from imutils.face_utils import FaceAligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put here the path to the model\n",
    "model = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/FacePipeline/faceDetection2.sav\", 'rb'))\n",
    "print(model)\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d650428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model \n",
    "voice_model = pickle.load(open('voice_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5499f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFaceAlign(img):\n",
    "  io.imsave('testt.jpg',img)\n",
    "  preds = fa.get_landmarks(img)\n",
    "  if preds == None:\n",
    "    print('Warning: No faces were detected.')\n",
    "    return None,0\n",
    "  if preds!=None:\n",
    "    mnXY= np.min(np.min(preds, axis=1), axis=0)\n",
    "    mxXY= np.max(np.max(preds, axis=1), axis=0)\n",
    "    mxY=int(mxXY[0]+2)\n",
    "    mnX=int(mnXY[1])-8\n",
    "    mxX=int(mxXY[1])\n",
    "    if mnX < 0:\n",
    "       mnX=0\n",
    "    if int(mxXY[0]+2) >= img.shape[1]:\n",
    "      mxY=img.shape[1]-1\n",
    "    if (img.shape[0]-2-mnX) <= 40:\n",
    "      return img[mnX:img.shape[0]-2,int(mnXY[0])-2:mxY],1\n",
    "    else:\n",
    "      return img[mnX:mnX+40,int(mnXY[0])-2:mxY],1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "    pick = []\n",
    "    x1 = boxes[:,2]\n",
    "    y1 = boxes[:,0]\n",
    "    x2 = boxes[:,3]\n",
    "    y2 = boxes[:,1]\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "    return boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec24f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(image):\n",
    "    boxes=[[i,i+k,j,int(j+k)] for k in [35,45,55] for i in range(0,image.shape[0]-k,5) for j in range(0,image.shape[1]-k,5) if (model.predict([feature.hog(transform.resize(image[i:i+k,j:int(j+k)],(48,48)))])[0] == 1)]\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9476384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "from torch.nn.modules.activation import ReLU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def SeparableConv2D(in_channels, out_channels, kernel=3):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, in_channels, kernel_size=kernel, stride=1, groups=in_channels,padding=1, bias=False),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "    )\n",
    "\n",
    "class ResidualXceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel=3):\n",
    "        super(ResidualXceptionBlock, self).__init__()\n",
    "        global device\n",
    "\n",
    "        self.depthwise_conv1 = SeparableConv2D(in_channels, out_channels, kernel).to(device)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.depthwise_conv2 = SeparableConv2D(out_channels, out_channels, kernel).to(device)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # self.padd = nn.ZeroPad2d(22)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, padding=22, bias=False)\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.residual_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # residual branch\n",
    "        residual = self.residual_conv(x)\n",
    "        residual = self.residual_bn(residual)\n",
    "        \n",
    "        # print('input',x.shape)\n",
    "        # feature extraction branch\n",
    "        x = self.depthwise_conv1(x)\n",
    "        # print('conv1',x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.depthwise_conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        # print('conv2',x.shape)\n",
    "\n",
    "        # x = self.padd(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x[:,:, 11:22, 11:22])\n",
    "        # print('max_pooling',x.shape)\n",
    "        # print('res',residual.shape)\n",
    "        return x + residual\n",
    "\n",
    "class Mini_Xception(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mini_Xception, self).__init__()\n",
    "        self.conv1 = conv_bn_relu(1, 8, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = conv_bn_relu(8, 8, kernel_size=3, stride=1, padding=0)\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            ResidualXceptionBlock(8 , 16).to(device),\n",
    "            ResidualXceptionBlock(16, 32).to(device),\n",
    "            ResidualXceptionBlock(32, 64).to(device),\n",
    "            ResidualXceptionBlock(64, 128).to(device)            \n",
    "        ])\n",
    "        self.conv3 = nn.Conv2d(128, 7, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "            # print('ith block', x.shape, block.device)\n",
    "\n",
    "        # print('blocks:',x.shape)\n",
    "        x = self.conv3(x)\n",
    "        # print('conv3',x.shape)\n",
    "        x = self.global_avg_pool(x)\n",
    "        # # x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_label_emotion(label : int) -> str:\n",
    "    label_emotion_map = { \n",
    "        0: 'Angry',\n",
    "        1: 'Disgust', \n",
    "        2: 'Fear', \n",
    "        3: 'Happy', \n",
    "        4: 'Sad', \n",
    "        5: 'Surprise', \n",
    "        6: 'Neutral'        \n",
    "    }\n",
    "    return label_emotion_map[label]\n",
    "\n",
    "\n",
    "def pr(img):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  mini_xception = Mini_Xception().to(device)\n",
    "  mini_xception.eval()\n",
    "\n",
    "  # Load model\n",
    "  # put here the path to the model\n",
    "  checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/weights_epoch_75.pth.tar')\n",
    "  mini_xception.load_state_dict(checkpoint['mini_xception'])\n",
    "\n",
    "    \n",
    "  input_face = cv2.resize(img, (48,48))\n",
    "  input_face = cv2.equalizeHist(input_face)\n",
    "  input_face = transforms.ToTensor()(input_face).to(device)\n",
    "  input_face = torch.unsqueeze(input_face, 0)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      input_face = input_face.to(device)\n",
    "      emotion = mini_xception(input_face)\n",
    "      # print(f'\\ntime={(time.time()-t) * 1000 } ms')\n",
    "\n",
    "      torch.set_printoptions(precision=6)\n",
    "      softmax = torch.nn.Softmax()\n",
    "      emotions_soft = softmax(emotion.squeeze()).reshape(-1,1).cpu().detach().numpy()\n",
    "      emotions_soft = np.round(emotions_soft, 3)\n",
    "      for i, em in enumerate(emotions_soft):\n",
    "          em = round(em.item(),3)\n",
    "          # print(f'{get_label_emotion(i)} : {em}')\n",
    "\n",
    "      emotion = torch.argmax(emotion)                \n",
    "      percentage = round(emotions_soft[emotion].item(), 2)\n",
    "      emotion = emotion.squeeze().cpu().detach().item()\n",
    "      emotion = get_label_emotion(emotion)\n",
    "      return emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2921901",
   "metadata": {},
   "source": [
    "# Voice analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(X, mfcc, chroma, mel,sample_rate):   \n",
    "    result=np.array([])\n",
    "    if mfcc:\n",
    "        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        stft=np.abs(librosa.stft(X))\n",
    "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa92cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_voice(file):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    y, s = librosa.load(file) # Downsample 44.1kHz to 8kHz\n",
    "    feature=extract_feature(X=y, mfcc=True, chroma=True, mel=True,sample_rate=s)\n",
    "    x.append(feature)\n",
    "    y_pred=voice_model.predict(x)\n",
    "    emotion = y_pred[0]\n",
    "    return emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask-ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50beadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "#this contains the path of folder that to store the images in\n",
    "app.config[\"IMAGE_UPLOADS\"] = \"/content\"\n",
    "run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "@app.route('/') \n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "    url = request.method\n",
    "    image = request.files[\"image\"]\n",
    "    \n",
    "    #the name of image file\n",
    "    imgName = image.filename\n",
    "    #save the image in colab directory /content \n",
    "    image.save(os.path.join(app.config[\"IMAGE_UPLOADS\"], image.filename))\n",
    "    frameTemp = cv2.imread(imgName,0)\n",
    "    frameTemp = cv2.resize(frameTemp, (100, 100))\n",
    "    rects = find(frameTemp)\n",
    "    rects = np.array(rects)\n",
    "    rects = non_max_suppression_fast(rects, 0.2)\n",
    "    output=\"\"\n",
    "    for rect in rects:\n",
    "        res,ret=getFaceAlign(frameTemp[rect[0]:rect[1],rect[2]:rect[3]])\n",
    "        if ret==1:\n",
    "          print(rect)\n",
    "          io.imshow(res)\n",
    "          print(pr(res))\n",
    "          output+= pr(res)+\" \"\n",
    "    \n",
    "    #return the emotion of the face's image to the html \n",
    "    return output\n",
    "\n",
    "@app.route('/predictVoice',methods=['POST'])\n",
    "def predictVoice():\n",
    "    audio = request.files[\"audio\"]\n",
    "    print(audio)\n",
    "    audiofile=audio.filename\n",
    "    #dir of audio is in the current directory \n",
    "    emotion = predict_voice(audiofile)\n",
    "    return emotion\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
