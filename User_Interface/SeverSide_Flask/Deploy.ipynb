{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Deploy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51137d49",
        "outputId": "675f30e6-b50c-4b01-8d6c-88192a2133f9"
      },
      "source": [
        "!pip install face-alignment"
      ],
      "id": "51137d49",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: face-alignment in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from face-alignment) (1.9.0+cu102)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from face-alignment) (0.16.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from face-alignment) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face-alignment) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from face-alignment) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from face-alignment) (0.51.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from face-alignment) (4.1.2.30)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->face-alignment) (3.7.4.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face-alignment) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face-alignment) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face-alignment) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face-alignment) (2.5.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face-alignment) (2.4.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->face-alignment) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->face-alignment) (57.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (1.3.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->face-alignment) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->face-alignment) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZRmCXyY55kk",
        "outputId": "da02398c-7aee-46fc-bab1-70a0f98100c1"
      },
      "source": [
        "pip install pydub"
      ],
      "id": "QZRmCXyY55kk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11f7c43d"
      },
      "source": [
        "%matplotlib inline\n",
        "import face_alignment\n",
        "from skimage import io,transform,data,color,feature\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import os\n",
        "import skimage.io as io\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import cross_val_score , GridSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "import numpy as np\n",
        "from numba import jit, cuda\n",
        "import pickle\n",
        "import timeit\n",
        "from os import listdir\n",
        "from matplotlib import image\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import string\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import dlib\n",
        "from imutils.face_utils import FaceAligner\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode"
      ],
      "id": "11f7c43d",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8481d410"
      },
      "source": [
        "import librosa\n",
        "import soundfile\n",
        "import glob\n",
        "import shutil\n",
        "import wave\n",
        "import contextlib\n",
        "from pydub import AudioSegment\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "id": "8481d410",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4be2c7d4",
        "outputId": "6925c9ca-4bb1-47a8-b3fd-77b767b82560"
      },
      "source": [
        "# put here the path to the mode\n",
        "model = pickle.load(open(\"faceDetection2.sav\", 'rb'))\n",
        "print(model)\n",
        "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True)"
      ],
      "id": "4be2c7d4",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearSVC from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d650428",
        "outputId": "6fcfaadf-e5c0-4744-d4c6-d406578b0c7f"
      },
      "source": [
        "#load the model \n",
        "voice_model = pickle.load(open('voice_model.pkl', 'rb'))"
      ],
      "id": "0d650428",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neural_network.multilayer_perceptron module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neural_network. Anything that cannot be imported from sklearn.neural_network is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.preprocessing.label module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.21.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.21.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5499f54"
      },
      "source": [
        "def getFaceAlign(img):\n",
        "  io.imsave('testt.jpg',img)\n",
        "  preds = fa.get_landmarks(img)\n",
        "  if preds == None:\n",
        "    #print('Warning: No faces were detected.')\n",
        "    return None,0\n",
        "  if preds!=None:\n",
        "    mnXY= np.min(np.min(preds, axis=1), axis=0)\n",
        "    mxXY= np.max(np.max(preds, axis=1), axis=0)\n",
        "    mxY=int(mxXY[0]+2)\n",
        "    mnX=int(mnXY[1])-8\n",
        "    mxX=int(mxXY[1])\n",
        "    if mnX < 0:\n",
        "       mnX=0\n",
        "    if int(mxXY[0]+2) >= img.shape[1]:\n",
        "      mxY=img.shape[1]-1\n",
        "    if (img.shape[0]-2-mnX) <= 40:\n",
        "      return img[mnX:img.shape[0]-2,int(mnXY[0])-2:mxY],1\n",
        "    else:\n",
        "      return img[mnX:mnX+50,int(mnXY[0])-2:mxY],1   # were 40 instead of 50"
      ],
      "id": "d5499f54",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7a7d03"
      },
      "source": [
        "def non_max_suppression_fast(boxes, overlapThresh):\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        "    pick = []\n",
        "    x1 = boxes[:,2]\n",
        "    y1 = boxes[:,0]\n",
        "    x2 = boxes[:,3]\n",
        "    y2 = boxes[:,1]\n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    idxs = np.argsort(y2)\n",
        "    while len(idxs) > 0:\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
        "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
        "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
        "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
        "        w = np.maximum(0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0, yy2 - yy1 + 1)\n",
        "        overlap = (w * h) / area[idxs[:last]]\n",
        "        idxs = np.delete(idxs, np.concatenate(([last],\n",
        "            np.where(overlap > overlapThresh)[0])))\n",
        "    return boxes[pick].astype(\"int\")"
      ],
      "id": "ba7a7d03",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ec24f65"
      },
      "source": [
        "def find(image):\n",
        "    boxes=[[i,i+k,j,int(j+k)] for k in [35,40,45,50] for i in range(0,image.shape[0]-k,10) for j in range(0,image.shape[1]-k,10) if (model.predict([feature.hog(transform.resize(image[i:i+k,j:int(j+k)],(48,48)))])[0] == 1)]\n",
        "    return boxes"
      ],
      "id": "7ec24f65",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9476384"
      },
      "source": [
        "import sys\n",
        "\n",
        "from torch.nn.modules.activation import ReLU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def SeparableConv2D(in_channels, out_channels, kernel=3):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, in_channels, kernel_size=kernel, stride=1, groups=in_channels,padding=1, bias=False),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "    )\n",
        "\n",
        "class ResidualXceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel=3):\n",
        "        super(ResidualXceptionBlock, self).__init__()\n",
        "        global device\n",
        "\n",
        "        self.depthwise_conv1 = SeparableConv2D(in_channels, out_channels, kernel).to(device)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.depthwise_conv2 = SeparableConv2D(out_channels, out_channels, kernel).to(device)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # self.padd = nn.ZeroPad2d(22)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        # self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, padding=22, bias=False)\n",
        "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.residual_bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # residual branch\n",
        "        residual = self.residual_conv(x)\n",
        "        residual = self.residual_bn(residual)\n",
        "        \n",
        "        # print('input',x.shape)\n",
        "        # feature extraction branch\n",
        "        x = self.depthwise_conv1(x)\n",
        "        # print('conv1',x.shape)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        x = self.depthwise_conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        # print('conv2',x.shape)\n",
        "\n",
        "        # x = self.padd(x)\n",
        "        x = self.maxpool(x)\n",
        "        # print(x[:,:, 11:22, 11:22])\n",
        "        # print('max_pooling',x.shape)\n",
        "        # print('res',residual.shape)\n",
        "        return x + residual\n",
        "\n",
        "class Mini_Xception(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mini_Xception, self).__init__()\n",
        "        self.conv1 = conv_bn_relu(1, 8, kernel_size=3, stride=1, padding=0)\n",
        "        self.conv2 = conv_bn_relu(8, 8, kernel_size=3, stride=1, padding=0)\n",
        "        self.residual_blocks = nn.ModuleList([\n",
        "            ResidualXceptionBlock(8 , 16).to(device),\n",
        "            ResidualXceptionBlock(16, 32).to(device),\n",
        "            ResidualXceptionBlock(32, 64).to(device),\n",
        "            ResidualXceptionBlock(64, 128).to(device)            \n",
        "        ])\n",
        "        self.conv3 = nn.Conv2d(128, 7, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        for block in self.residual_blocks:\n",
        "            x = block(x)\n",
        "            # print('ith block', x.shape, block.device)\n",
        "\n",
        "        # print('blocks:',x.shape)\n",
        "        x = self.conv3(x)\n",
        "        # print('conv3',x.shape)\n",
        "        x = self.global_avg_pool(x)\n",
        "        # # x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def get_label_emotion(label : int) -> str:\n",
        "    label_emotion_map = { \n",
        "        0: 'Angry',\n",
        "        1: 'Disgust', \n",
        "        2: 'Fear', \n",
        "        3: 'Happy', \n",
        "        4: 'Sad', \n",
        "        5: 'Surprise', \n",
        "        6: 'Neutral'        \n",
        "    }\n",
        "    return label_emotion_map[label]\n",
        "\n",
        "\n",
        "def pr(img):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  mini_xception = Mini_Xception().to(device)\n",
        "  mini_xception.eval()\n",
        "\n",
        "  # Load model\n",
        "  # put here the path to the model\n",
        "  checkpoint = torch.load('/content/drive/MyDrive/Colab Notebooks/weights_epoch_75.pth.tar')\n",
        "  mini_xception.load_state_dict(checkpoint['mini_xception'])\n",
        "\n",
        "    \n",
        "  input_face = cv2.resize(img, (48,48))\n",
        "  input_face = cv2.equalizeHist(input_face)\n",
        "  input_face = transforms.ToTensor()(input_face).to(device)\n",
        "  input_face = torch.unsqueeze(input_face, 0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      input_face = input_face.to(device)\n",
        "      emotion = mini_xception(input_face)\n",
        "      # print(f'\\ntime={(time.time()-t) * 1000 } ms')\n",
        "\n",
        "      torch.set_printoptions(precision=6)\n",
        "      softmax = torch.nn.Softmax()\n",
        "      emotions_soft = softmax(emotion.squeeze()).reshape(-1,1).cpu().detach().numpy()\n",
        "      emotions_soft = np.round(emotions_soft, 3)\n",
        "      for i, em in enumerate(emotions_soft):\n",
        "          em = round(em.item(),3)\n",
        "          # print(f'{get_label_emotion(i)} : {em}')\n",
        "\n",
        "      emotion = torch.argmax(emotion)                \n",
        "      percentage = round(emotions_soft[emotion].item(), 2)\n",
        "      emotion = emotion.squeeze().cpu().detach().item()\n",
        "      emotion = get_label_emotion(emotion)\n",
        "      return emotion"
      ],
      "id": "e9476384",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2921901"
      },
      "source": [
        "# Voice analysis"
      ],
      "id": "c2921901"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e98c0aeb"
      },
      "source": [
        "#DataFlair - Extract features (mfcc, chroma, mel) from a sound file\n",
        "def extract_feature(X, mfcc, chroma, mel,sample_rate):   \n",
        "    result=np.array([])\n",
        "    if mfcc:\n",
        "        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "        result=np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "        stft=np.abs(librosa.stft(X))\n",
        "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "        result=np.hstack((result, chroma))\n",
        "    if mel:\n",
        "        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "        result=np.hstack((result, mel))\n",
        "    return result"
      ],
      "id": "e98c0aeb",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaa92cd3"
      },
      "source": [
        "def predict_voice(file):\n",
        "    x=[]\n",
        "    y=[]\n",
        "    y, s = librosa.load(file) # Downsample 44.1kHz to 8kHz\n",
        "    feature=extract_feature(X=y, mfcc=True, chroma=True, mel=True,sample_rate=s)\n",
        "    x.append(feature)\n",
        "    y_pred=voice_model.predict(x)\n",
        "    emotion = y_pred[0]\n",
        "    return emotion"
      ],
      "id": "eaa92cd3",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QphZfZz6aiT"
      },
      "source": [
        "## Split Audio"
      ],
      "id": "6QphZfZz6aiT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHDxh4ne6edI"
      },
      "source": [
        "def get_duration(fname):\n",
        "    with contextlib.closing(wave.open(fname,'r')) as f:\n",
        "        frames = f.getnframes()\n",
        "        rate = f.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "        return duration"
      ],
      "id": "sHDxh4ne6edI",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQtWZYvH6g77"
      },
      "source": [
        "def get_emotion_dic(fname):\n",
        "  duration = get_duration(fname) \n",
        "  \n",
        "  # frames/ is the directory to stores audio frames\n",
        "  dir='frames'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "  else:\n",
        "    shutil.rmtree(dir)           \n",
        "    os.makedirs(dir)  \n",
        "  emotion_dic={'bad':0,'medium':0,'good':0}\n",
        "  originalAudio = AudioSegment.from_wav(fname)\n",
        "  for i in range(0,int(duration),3):\n",
        "      #newAudio = AudioSegment.from_wav(\"ted2.wav\")\n",
        "      newAudio = originalAudio[i*1000:(i+3)*1000]\n",
        "      newAudio.export('frames/'+str(i)+'.wav', format=\"wav\") #Exports to a wav file in the current path.\n",
        "      emotion = predict_voice('frames/'+str(i)+'.wav') \n",
        "      emotion_dic[emotion]+=1\n",
        "  return emotion_dic"
      ],
      "id": "JQtWZYvH6g77",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nuLkGRrIKrr"
      },
      "source": [
        "# **Answers Similarity**\n"
      ],
      "id": "3nuLkGRrIKrr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP7WA-htIXVw",
        "outputId": "52ee2839-dbc6-452c-e7cb-c8e478b35bf7"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "!pip install pytorch-pretrained-bert\n",
        "from pytorch_pretrained_bert import BertModel"
      ],
      "id": "rP7WA-htIXVw",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.9.0+cu102)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.18.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.5.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.21.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.1->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.1->boto3->pytorch-pretrained-bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eaw5DZ5Ic6l"
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "def clean2_text(text):\n",
        "  #remove some of stopwords as 'a, an, the'\n",
        "  txt = clean_text(text)\n",
        "  words = txt.split(' ')\n",
        "  aft_remove = [w for w in words if w not in ['a', 'an', 'the','of', 'that', '']]\n",
        "  return ' '.join(aft_remove)"
      ],
      "id": "4eaw5DZ5Ic6l",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCdeXPhUIfVf"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "id": "UCdeXPhUIfVf",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7g8AzT8Ih9M"
      },
      "source": [
        "class Similarity_Model(nn.Module):\n",
        "    def __init__(self, output_dim, n_layers, hidden_dim, freeze_bert):\n",
        "        super(Similarity_Model,self).__init__()\n",
        " \n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "        \n",
        "        #bert Model and Freeze the BERT model\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_model.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        #LSTM layers\n",
        "        # self.lstm = nn.LSTM(768, hidden_dim, n_layers, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # linear layer\n",
        "        self.fc = nn.Linear(768, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_masks, token_type_ids, hidden):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        sequence_output, pooled_output = self.bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids) \n",
        "        # print(\"seq  \" , len(sequence_output))\n",
        "        \n",
        "        # lstm_out, hidden = self.lstm(sequence_output[0], hidden)\n",
        "\n",
        "        # lstm_out = lstm_out.permute(0,2,1)\n",
        "        \n",
        "        # out_max = F.max_pool1d(lstm_out, kernel_size=lstm_out.shape[2])\n",
        "        # out_avg = F.avg_pool1d(lstm_out, kernel_size=lstm_out.shape[2])\n",
        "        \n",
        "        # out = torch.cat((out_avg, out_max), dim=1)\n",
        "        # out = out.permute(0,2,1)\n",
        "\n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(pooled_output)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        out = sig_out.view(batch_size, -1)\n",
        "\n",
        "        out = out[:, -1]\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # initialize hidden states with sizes n_layers x batch_size x hidden_dim\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ],
      "id": "K7g8AzT8Ih9M",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JWsg5ARnxmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefa6e8d-c709-4c8e-d5de-1d24999d3e5f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "1JWsg5ARnxmT",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qczUheufIjBS",
        "outputId": "4633dd48-1915-4f7b-b8d0-7295d8ad2697"
      },
      "source": [
        "no_layers = 1\n",
        "output_dim = 1\n",
        "hidden_dim = 128\n",
        "Freeze_bert = False\n",
        "Similar_Model = Similarity_Model(output_dim, no_layers, hidden_dim, Freeze_bert)\n",
        "#moving to gpu\n",
        "Similar_Model.to(device)\n",
        "Similar_Model.eval()\n",
        "\n",
        "Similar_Model.load_state_dict(torch.load(\"/content/drive/MyDrive/Snli_TechSimilarity.pt\"))"
      ],
      "id": "qczUheufIjBS",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsvETCozIpK9"
      },
      "source": [
        "def predict_similarity(ans1, ans2):\n",
        "  sentence1 = clean2_text(ans1)\n",
        "  sentence2 = clean2_text(ans2)\n",
        "  print(sentence1, sentence2)\n",
        "  encoded_pair = tokenizer(  sentence1, sentence2, \n",
        "      add_special_tokens=True,\n",
        "      return_tensors='pt'  # Return torch.Tensor objects\n",
        "  )\n",
        "  text, attention, token_ids = encoded_pair['input_ids'].expand(1,-1), encoded_pair['attention_mask'].expand(1,-1), encoded_pair['token_type_ids'].expand(1,-1)\n",
        "  inputs, attention, token_ids = text.to(device), attention.to(device), token_ids.to(device)\n",
        "  h = Similar_Model.init_hidden(1)\n",
        "  h = tuple([each.data for each in h])\n",
        "  output, _ = Similar_Model(inputs, attention, token_ids, h)\n",
        "  return str(output.item())"
      ],
      "id": "tsvETCozIpK9",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQCdQbv_Z5Ih"
      },
      "source": [
        "# **Speech Recognition**"
      ],
      "id": "LQCdQbv_Z5Ih"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUmRNecnaA9h",
        "outputId": "1ef8c1b8-f766-4f74-e781-dc197061e11c"
      },
      "source": [
        "!pip install google-cloud\n",
        "!pip install google-cloud-speech"
      ],
      "id": "UUmRNecnaA9h",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud in /usr/local/lib/python3.7/dist-packages (0.34.0)\n",
            "Requirement already satisfied: google-cloud-speech in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: proto-plus>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-speech) (1.19.0)\n",
            "Requirement already satisfied: libcst>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-speech) (0.3.19)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-speech) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-speech) (21.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from proto-plus>=1.4.0->google-cloud-speech) (3.17.3)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.2.5->google-cloud-speech) (5.4.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.2.5->google-cloud-speech) (0.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.2.5->google-cloud-speech) (3.7.4.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (2018.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (57.2.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (1.32.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (1.34.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-speech) (2.4.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-speech) (0.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (2021.5.30)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (4.7.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.26.0->google-cloud-speech) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFP5ppS8aFEn"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import io as ar\n",
        "import os\n",
        "from google.cloud import speech"
      ],
      "id": "uFP5ppS8aFEn",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z62TAS94aQ3Z"
      },
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'avian-tract-283207-f1553ac44767.json'\n",
        "client = speech.SpeechClient()"
      ],
      "id": "Z62TAS94aQ3Z",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nvfNhB2amE3"
      },
      "source": [
        "def SpeechRecognition(audiof):\n",
        "  x, s = librosa.load(audiof)\n",
        "  soundfile.write('tmp.wav', x, s)\n",
        "  with ar.open('/content/tmp.wav','rb') as audio_file:\n",
        "    content = audio_file.read()\n",
        "  audio = speech.RecognitionAudio(content=content)\n",
        "  config = speech.RecognitionConfig(\n",
        "      encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
        "      language_code = 'en-US')\n",
        "  operation = client.long_running_recognize(config=config, audio=audio)\n",
        "  print(\"Waiting for operation to complete...\")\n",
        "  response = operation.result(timeout=90)\n",
        "  answer = \"\"\n",
        "  for result in response.results:\n",
        "      # print('Transcript: {}'.format(result.alternatives[0].transcript))\n",
        "      answer += result.alternatives[0].transcript\n",
        "  return answer    "
      ],
      "id": "4nvfNhB2amE3",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1cBO8pnIvBp"
      },
      "source": [
        "# **Routes**\n"
      ],
      "id": "p1cBO8pnIvBp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8720c670",
        "outputId": "a30bb326-1c46-4ab8-c117-4fb473d4980a"
      },
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install -U flask-cors"
      ],
      "id": "8720c670",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already up-to-date: flask-cors in /usr/local/lib/python3.7/dist-packages (3.0.10)\n",
            "Requirement already satisfied, skipping upgrade: Flask>=0.9 in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.9->flask-cors) (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50beadcd",
        "outputId": "aa353bf7-4502-4821-f539-5f77f87760fe"
      },
      "source": [
        "from flask import Flask, request, jsonify, render_template\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "#this contains the path of folder that to store the images in\n",
        "app.config[\"IMAGE_UPLOADS\"] = \"/content\"\n",
        "app.config[\"AUDIO_UPLOADS\"]=\"/content\"\n",
        "run_with_ngrok(app)  # Start ngrok when app is run\n",
        "\n",
        "#@app.route('/') \n",
        "#def home():\n",
        "#    return render_template('index.html')\n",
        "\n",
        "@app.route(\"/predict\", methods=['post'])\n",
        "def predict():\n",
        "    print(\"recieved data: \", request.form[\"image\"])\n",
        "\n",
        "    image_bytes = b64decode(request.form[\"image\"].split(',')[1])\n",
        "    # convert bytes to numpy array\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    # decode numpy array into OpenCV BGR image\n",
        "    image = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "    url = request.method\n",
        "    frameTemp = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    frameTemp = cv2.resize(frameTemp, (130, 130))\n",
        "    try:\n",
        "      rects = find(frameTemp)\n",
        "      rects = np.array(rects)\n",
        "      rects = non_max_suppression_fast(rects, 0.2)\n",
        "      output=\"\"\n",
        "      for rect in rects: \n",
        "          res,ret=getFaceAlign(frameTemp[rect[0]:rect[1],rect[2]:rect[3]])\n",
        "          if ret==1:\n",
        "            #print(rect)\n",
        "            io.imshow(res)\n",
        "            pred = pr(res)\n",
        "            ans = None\n",
        "            if pred in ['Disgust','Fear','Sad','Angry']:\n",
        "              ans = 'bad'\n",
        "            elif pred in ['Surprise' , 'Neutral']:\n",
        "              ans = 'medium'\n",
        "            elif pred == 'Happy':\n",
        "              ans = 'good'             \n",
        "            output+= ans+\" \"\n",
        "            print(ans)\n",
        "      #return the emotion of the face's image to the html\n",
        "      if output!=\"\":\n",
        "        return jsonify(output)\n",
        "      return jsonify(\"medium\")\n",
        "    \n",
        "    except:\n",
        "        return jsonify(\"medium\")\n",
        "\n",
        "@app.route('/predictVoice',methods=['POST'])\n",
        "def predictVoice():\n",
        "    audio = request.files[\"file\"]\n",
        "    audiofile=audio.filename\n",
        "    #save the audio in colab directory /content \n",
        "    audio.save(os.path.join(app.config[\"AUDIO_UPLOADS\"], audiofile))\n",
        "    #dir of audio is in the current directory \n",
        "    #convert audio to correct wav file\n",
        "    newAudio = AudioSegment.from_file(audiofile)\n",
        "    #overwrite corrupted audio with the corrected audio \n",
        "    newAudio.export(audiofile, format=\"wav\")\n",
        "    emotion_dic=get_emotion_dic(\"/content/\"+audiofile)\n",
        "    return emotion_dic\n",
        "\n",
        "@app.route('/predictSimilarity', methods=['POST'])\n",
        "def predictSimilarity():\n",
        "    audio = request.files[\"file\"]\n",
        "    audiofile=audio.filename\n",
        "    audio.save(os.path.join(app.config[\"AUDIO_UPLOADS\"], audio.filename))\n",
        "    # #convert audio to correct wav file\n",
        "    # newAudio = AudioSegment.from_file(audiofile)\n",
        "    # #overwrite corrupted audio with the corrected audio \n",
        "    # newAudio.export(audiofile, format=\"wav\")\n",
        "\n",
        "    answer1 = request.form['ans1']\n",
        "    print(answer1)\n",
        "\n",
        "    answer2 = SpeechRecognition(\"/content/\"+audiofile)\n",
        "    print(answer2)\n",
        "    prob = predict_similarity(answer1, answer2)\n",
        "    return prob    \n",
        "\n",
        "app.run()"
      ],
      "id": "50beadcd",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://8fa6cc161936.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "Overfitting is the most common issue which occurs in deep learning. It usually occurs when a deep learning algorithm apprehends the sound of specific data. It also appears when the particular algorithm is well suitable for the data and shows up when the algorithm or model represents high variance and low bias.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
            "127.0.0.1 - - [18/Jul/2021 09:39:53] \"\u001b[37mPOST /predictVoice HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for operation to complete...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [18/Jul/2021 09:40:01] \"\u001b[37mPOST /predictSimilarity HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Transcript: overfitting is a big problem\n",
            "Transcript:  in the Deep learning\n",
            "Transcript:  that's decreed that\n",
            "Transcript:  before mansabs Ahmad\n",
            "overfitting is a big problem in the Deep learning that's decreed that before mansabs Ahmad\n",
            "overfitting is most common issue occurs in deep learning it usually occurs when deep learning algorithm apprehends sound specific data it also appears when particular algorithm is well suitable for data and shows up when algorithm or model represents high variance and low bias overfitting is big problem in deep learning thats decreed before mansabs ahmad\n",
            "Deep learning model takes longer time to execute the model. In some cases, it even takes several days to execute a single model depends on complexity. The deep learning model is not good for small data sets, and it fails here.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
            "127.0.0.1 - - [18/Jul/2021 09:41:22] \"\u001b[37mPOST /predictVoice HTTP/1.1\u001b[0m\" 200 -\n",
            "[2021-07-18 09:41:25,366] ERROR in app: Exception on /predictSimilarity [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/api_core/grpc_helpers.py\", line 73, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/grpc/_channel.py\", line 923, in __call__\n",
            "    return _end_unary_response_blocking(state, call, False, None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/grpc/_channel.py\", line 826, in _end_unary_response_blocking\n",
            "    raise _InactiveRpcError(state)\n",
            "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.INVALID_ARGUMENT\n",
            "\tdetails = \"Inline audio exceeds duration limit. Please use a GCS URI.\"\n",
            "\tdebug_error_string = \"{\"created\":\"@1626601285.365741577\",\"description\":\"Error received from peer ipv4:74.125.135.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"Inline audio exceeds duration limit. Please use a GCS URI.\",\"grpc_status\":3}\"\n",
            ">\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 2447, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask_cors/extension.py\", line 165, in wrapped_function\n",
            "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 1821, in handle_user_exception\n",
            "    reraise(exc_type, exc_value, tb)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/_compat.py\", line 39, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/flask/app.py\", line 1936, in dispatch_request\n",
            "    return self.view_functions[rule.endpoint](**req.view_args)\n",
            "  File \"<ipython-input-27-9935354accef>\", line 84, in predictSimilarity\n",
            "    answer2 = SpeechRecognition(\"/content/\"+audiofile)\n",
            "  File \"<ipython-input-25-3b5c20d76344>\", line 10, in SpeechRecognition\n",
            "    operation = client.long_running_recognize(config=config, audio=audio)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/cloud/speech_v1/services/speech/client.py\", line 503, in long_running_recognize\n",
            "    response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/api_core/gapic_v1/method.py\", line 145, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/api_core/grpc_helpers.py\", line 75, in error_remapped_callable\n",
            "    six.raise_from(exceptions.from_grpc_error(exc), exc)\n",
            "  File \"<string>\", line 3, in raise_from\n",
            "google.api_core.exceptions.InvalidArgument: 400 Inline audio exceeds duration limit. Please use a GCS URI.\n",
            "127.0.0.1 - - [18/Jul/2021 09:41:25] \"\u001b[35m\u001b[1mPOST /predictSimilarity HTTP/1.1\u001b[0m\" 500 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data normalization is an essential preprocessing step, which is used to rescale values to fit in a specific range. It assures better convergence during backpropagation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
            "127.0.0.1 - - [18/Jul/2021 09:41:36] \"\u001b[37mPOST /predictVoice HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for operation to complete...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [18/Jul/2021 09:41:44] \"\u001b[37mPOST /predictSimilarity HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Transcript: Nathan normalization is nothing there but\n",
            "Transcript:  makers alley toward platten\n",
            "Nathan normalization is nothing there but makers alley toward platten\n",
            "data normalization is essential preprocessing step is used to rescale values to fit in specific range it assures better convergence during backpropagation nathan normalization is nothing there but makers alley toward platten\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [18/Jul/2021 09:42:37] \"\u001b[37mPOST /predictVoice HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "If the set of weights in the network is put to a zero, then all the neurons at each layer will start producing the same output and the same gradients during backpropagation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for operation to complete...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [18/Jul/2021 09:42:47] \"\u001b[37mPOST /predictSimilarity HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "if set weights in network is put to zero then all neurons at each layer will start producing same output and same gradients during backpropagation \n",
            "The input layer contains input neurons which send information to the hidden layer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
            "127.0.0.1 - - [18/Jul/2021 09:42:54] \"\u001b[37mPOST /predictVoice HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for operation to complete...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [18/Jul/2021 09:43:04] \"\u001b[37mPOST /predictSimilarity HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Transcript: in which layer is the last layer in the network\n",
            "Transcript:  modest\n",
            "in which layer is the last layer in the network modest\n",
            "input layer contains input neurons send information to hidden layer in layer is last layer in network modest\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}