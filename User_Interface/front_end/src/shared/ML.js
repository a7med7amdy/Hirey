export const ML = 
[
    {
        id: 0,
        question: "what is the machine learning ?",
        answer1: "Machine learning is a form of AI that enables a system to learn from data rather than through explicit programming. However, machine learning is not a simple process. As the algorithms ingest training data, it is then possible to produce more precise models based on that data. A machine-learning model is the output generated when you train your machine-learning algorithm with data. After training, when you provide a model with an input, you will be given an output. For example, a predictive algorithm will create a predictive model. Then, when you provide the predictive model with data, you will receive a prediction based on the data that trained the model.",
        answer2: "Machine learning is the study of computer algorithms that improve automatically through experience.It is seen as a subset of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.",
        answer3: "At a very high level, machine learning is the process of teaching a computer system how to make accurate predictions when fed data."
    },
    {
        id: 1,
        question: "What is ‘training Set’ and ‘test Set’ in a Machine Learning Model? ",
        answer1: "The training set is examples given to the model to analyze and learn.The test set is used to test the accuracy of the hypothesis generated by the model",
        answer2: "We split the given data set into two different sections namely,’Training set’ and ‘Test Set’. ‘Training set’ is the portion of the dataset used to train the model. ‘Testing set’ is the portion of the dataset used to test the trained model.",
        answer3: "In a dataset, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built."
        
    },
    {
        id: 2,
        question: "What Is a False Positive and False Negative and How Are They Significant?",
        answer1: "False positives are those cases which wrongly get classified as True but are False.False negatives are those cases which wrongly get classified as False but are True.",
        answer2: "A false positive is an outcome where the model incorrectly predicts the positive class.false negative is an outcome where the model incorrectly predicts the negative class.",
        answer3: "False negative: When a data point is classified as a negative example(say class 0) but it is actually a positive example(belongs to class 1).False positive: When a data point is classified as a positive example(say class 1) but it is actually a negative example(belongs to class 0)."
    },
    {
        id: 3,
        question: "What is a Confusion Matrix?",
        answer1: "A confusion matrix is a technique for summarizing the performance of a classification algorithm.",
        answer2: "A much better way to evaluate the performance of a classifier is to look at the confusion matrix.",
        answer3: "a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm,"
    },
    {
        id: 4,
        question: "what is the trade off between bias and variance ?",
        answer1: "If the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias.",
        answer2: "predicitve models have a tradeoff between bias (how well the model fits the data) and variance (how much the model changes based on changes In the inputs). Simpler models are stable (low variance) but they don't get close to the truth (high bias) more complex models are more prone to overfitting (high variance) but they are expressive enough to get close to the truth (low bias) . The best model for a given problem usually lies somewhere in the middle.",
        answer3: "Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance."
    },
    {
        id: 5,
        question: "what is the difference between classification and regression ?",
        answer1: "Classification is used to produce discrete results, classification is used to classify data into some specific categories. For example, classifying emails into spam and non-spam categories. Whereas, We use regression analysis when we are dealing with continuous data, for example predicting stock prices at a certain point in time",
        answer2: "Fundamentally, classification is about predicting a label and regression is about predicting a quantity",
        answer3: "The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms."
    },
    {
        id: 6,
        question: "What is the use of the Decision Tree?",
        answer1: "A decision tree is used to explain the sequence of actions that must be performed to get the desired output. It is a hierarchical diagram that shows the actions.",
        answer2: "A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.",
        answer3: "decision tree is a type of flowchart that shows a clear pathway to a decision. In terms of data analytics, it is a type of algorithm that includes conditional ‘control’ statements to classify data. A decision tree starts at a single point (or ‘node’) which then branches (or ‘splits’) in two or more directions. Each branch offers different possible outcomes, incorporating a variety of decisions and chance events until a final outcome is achieved. When shown visually, their appearance is tree-like"
    },
    {
        id: 7,
        question: "What are the parametric models? Give an example.",
        answer1: "Parametric models are those with a finite number of parameters. To predict new data, you only need to know the parameters of the model. Examples include linear regression, logistic regression, and linear SVMs.",
        answer2: "A Parametric Model is a concept used in statistics to describe a model in which all its information is represented within its parameters. In short, the only information needed to predict future or unknown values from the current value is the parameters. Parametric models often deal with discrete values,",
        answer3: "In machine learning, a parametric model is any model that captures all the information about its predictions within a finite set of parameters. Sometimes the model must be trained to select its parameters, as in the case of neural networks. Sometimes the parameters are selected by hand or through a simple calculation process.",
    },
    {
        id: 8,
        question: "What Is Semi-supervised Machine Learning?",
        answer1: "In the case of semi-supervised learning, the training data contains a small amount of labeled data and a large amount of unlabeled data.",
        answer2: "the algorithm is trained upon a combination of labeled and unlabeled data. Typically, this combination will contain a very small amount of labeled data and a very large amount of unlabeled data.",
        answer3: "In semi-supervised learning, an algorithm learns from a dataset that includes both labeled and unlabeled data, usually mostly unlabeled."
    },
    {
        id: 9,
        question: "What are support vector machines?",
        answer1: "Support vector machines (SVMs) are powerful yet flexible supervised machine learning algorithms which are used both for classification and regression. But generally, they are used in classification problems.",
        answer2: "Support vector machines are supervised learning algorithms used for classification and regression analysis.",
        answer3: "A support vector machine (SVM) is a computer algorithm that learns by example to assign labels to objects"
    },
    {
        id: 10,
        question: "What is Linear Regression?",
        answer1: "Linear Regression is a supervised Machine Learning algorithm. It is used to find the linear relationship between the dependent and the independent variables for predictive analysis.",
        answer2: "Simple linear regression is a type of regression analysis where the number of independent variables is one and there is a linear relationship between the independent(x) and dependent(y) variable.",
        answer3: "Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable."
    },
    {
        id: 11,
        question: "What is data augmentation? Can you give some examples?",
        answer1: "Data augmentation is a technique for synthesizing new data by modifying existing data in such a way that the target is not changed, or it is changed in a known way. CV is one of the fields where data augmentation is very useful. There are many modifications that we can do to images: Resize, Horizontal or vertical flip, Rotate, noise, Deform, or Modify colors. For example, on OCR, doing flips will change the text and won’t be beneficial; however, resizes and small rotations may help.",
        answer2: "Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model.",
        answer3: "Data augmentation adds value to base data by adding information derived from internal and external sources within an enterprise."
    },
    {
        id: 12,
        question: "Define entropy?",
        answer1: "Entropy is the measure of uncertainty associated with random variable Y. It is the expected number of bits required to communicate the value of the variable.",
        answer2: "Entropy is nothing but the measure of disorder.",
        answer3: "Entropy measures uncertainty"
    },
    {
        id: 13,
        question: "What is the ROC Curve ",
        answer1: "The ROC (receiver operating characteristic) the performance plot for binary classifiers of True Positive Rate y-axis vs. False Positive Rate x-axis",
        answer2: "The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).",
        answer3: "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate and false Positive Rate"
    },
    {
        id: 14,
        question: "How do you handle outliers in the data?",
        answer1: "Outlier is an observation in the data set that is far away from other observations in the data set. We can discover outliers using tools and functions like box plot, scatter plot, Z-Score, IQR score etc. and then handle them based on the visualization we have got. To handle outliers, we can cap at some threshold, use transformations to reduce skewness of the data and remove outliers if they are anomalies or errors.",
        answer2: "1. Drop the outlier records. In the case of Bill Gates, or another true outlier, sometimes it is best to completely remove that record from your dataset to keep that person or event from skewing your analysis. 2. Cap your outliers data. Another way to handle true outliers is to cap them. For example, if you are using income, you might find that people above a certain income level behave in the same way as those with a lower income. In this case, you can cap the income value at a level that keeps that intact. 3. Assign a new value. If an outlier seems to be due to a mistake in your data, you try imputing a value. Common imputation methods include using the mean of a variable or utilizing a regression model to predict the missing value.",
        answer3: "Univariate method: This method looks for data points with extreme values on one variable. Multivariate method: Here, we look for unusual combinations of all the variables. Minkowski error: This method reduces the contribution of potential outliers in the training process"
    },
    {
        id: 15,
        question: "What is the difference between inductive machine learning and deductive machine learning?",
        answer1: "The difference between inductive machine learning and deductive machine learning are as follows: machine-learning where the model learns by examples from a set of observed instances to draw a generalized conclusion whereas in deductive learning the model first draws the conclusion and then the conclusion is drawn.",
        answer2: "Deductive reasoning moves from generalized statement to a valid conclusion, whereas Inductive reasoning moves from specific observation to a generalization.",
        answer3: "inductive machine learning : Observe and learn from the set of instances and then draw the conclusion.It is Statistical machine learning like KNN (K-nearest neighbour) or SVM (Support Vector Machine).Deductive Machine Learning: Derives conclusion and then work on it based on the previous decision.Machine learning algorithm to deductive reasoning using a decision tree"
    },
    {
        id: 16,
        question: "What do you mean by Associative Rule Mining (ARM)?",
        answer1: "Associative Rule Mining is one of the techniques to discover patterns in data like features (dimensions) which occur together and features (dimensions) which are correlated.",
        answer2: "Association rule mining finds interesting associations and relationships among large sets of data items. This rule shows how frequently a itemset occurs in a transaction. A typical example is Market Based Analysis.",
        answer3: "Association Rule Mining is one of the ways to find patterns in data. It finds: features (dimensions) which occur together and features (dimensions) which are correlated"
    },
    {
        id: 17,
        question: "Explain Ensemble learning",
        answer1: "In ensemble learning, many base models like classifiers and regressors are generated and combined together so that they give better results. It is used when we build component classifiers that are accurate and independent. There are sequential as well as parallel ensemble methods.",
        answer2: "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[1][2][3] Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.",
        answer3: "Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Ensemble learning is primarily used to improve the (classification, prediction, function approximation, etc.) performance of a model, or reduce the likelihood of an unfortunate selection of a poor one."
    },
    {
        id: 18,
        question: "State the differences between causality and correlation?",
        answer1: "Causality applies to situations where one action, say X, causes an outcome, say Y, whereas Correlation is just relating one action (X) to another action(Y) but X does not necessarily cause Y.",
        answer2: "Correlation tests for a relationship between two variables. However, seeing two variables moving together does not necessarily mean we know whether one variable causes the other to occur. This is why we commonly say “correlation does not imply causation.”",
        answer3: "While causation and correlation can exist at the same time, correlation does not imply causation. Causation explicitly applies to cases where action A {quote:right}Causation explicitly applies to cases where action A causes outcome B.{/quote} causes outcome B. On the other hand, correlation is simply a relationship. Action A relates to Action B—but one event doesn’t necessarily cause the other event to happen."
    },
    {
        id: 19,
        question: "Define precison and recall ?",
        answer1: "precision is the ratio of serveral events you can correctly recall to the total number of events you recall  , precision = (True Positive)/(True Positive + False Negative), recall is the ratio of a number of events you can recall the total events , recall = (True Positive)/(True Positive + False Negative)",
        answer2: "precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were actually retrieved. Both precision and recall are therefore based on an understanding and measure of relevance.",
        answer3: "Precision attempts to answer the following question: What proportion of positive identifications was actually correct? Recall attempts to answer the following question: What proportion of actual positives was identified correctly?"
    }
]